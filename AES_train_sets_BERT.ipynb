{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o17F95XwDImo"
   },
   "source": [
    "# Automated Essay Scoring \n",
    "\n",
    "### What is this Notebook about?\n",
    "\n",
    "- This Notebook contains all raw results and experiments done with the various deep learning methods. The output cells contains the results that we got. The whole Notebook may take approximately 5-6 hours. This Notebook contains the following:\n",
    "\n",
    "* Preprocess data\n",
    "* Important util methods to be used later\n",
    "* Deep Learning Model declaration\n",
    "* Running bert for individual sets\n",
    "* Running bert for whole dataset\n",
    "* Running word2vec for individual sets\n",
    "* Running word2vec for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "0B76LD1O7uP1",
    "outputId": "d52fe13a-006f-466f-8911-b31ebe8de554"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Word2Vec\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KFold\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# import important libraries and download data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import multiprocessing\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "# ! git clone https://github.com/Gaurav-Pande/AES_DL.git && mv AES_DL/data .\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDmr9sRxmWeW"
   },
   "outputs": [],
   "source": [
    "# Declaring some visualization methods to plot accuracy and model diagram\n",
    "def plot_accuracy_curve(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_acrchitecture(filename, model):\n",
    "    plot_model(model, to_file=str(filename) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnA5ciNY8Fhj"
   },
   "outputs": [],
   "source": [
    "# method to split data into sets\n",
    "def split_in_sets(data):\n",
    "    essay_sets = []\n",
    "    min_scores = []\n",
    "    max_scores = []\n",
    "    \n",
    "    for s in range(1,9):\n",
    "        essay_set = data[data[\"essay_set\"] == s]\n",
    "        essay_set.dropna(axis=1, inplace=True)\n",
    "        n, d = essay_set.shape\n",
    "        set_scores = essay_set[\"domain1_score\"]\n",
    "        print (\"Set\", s, \": Essays = \", n , \"\\t Attributes = \", d)\n",
    "        min_scores.append(set_scores.min())\n",
    "        max_scores.append(set_scores.max())\n",
    "        essay_sets.append(essay_set)\n",
    "    \n",
    "    return (essay_sets, min_scores, max_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_yJZ-g5GVQd"
   },
   "source": [
    "In the below cell, we can see the data we need to operate. We essentially drops the column, we dont need and keep the domain_score only along with essay text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "colab_type": "code",
    "id": "8CwSNzeK8J38",
    "outputId": "0a741508-c85c-43da-d088-5e3ea12e311a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               4               4             NaN              8   \n1               5               4             NaN              9   \n2               4               3             NaN              7   \n3               5               5             NaN             10   \n4               4               4             NaN              8   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>rater1_domain1</th>\n      <th>rater2_domain1</th>\n      <th>rater3_domain1</th>\n      <th>domain1_score</th>\n      <th>rater1_domain2</th>\n      <th>rater2_domain2</th>\n      <th>domain2_score</th>\n      <th>...</th>\n      <th>rater2_trait3</th>\n      <th>rater2_trait4</th>\n      <th>rater2_trait5</th>\n      <th>rater2_trait6</th>\n      <th>rater3_trait1</th>\n      <th>rater3_trait2</th>\n      <th>rater3_trait3</th>\n      <th>rater3_trait4</th>\n      <th>rater3_trait5</th>\n      <th>rater3_trait6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>5</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"./data/training_set_rel3.tsv\"\n",
    "data = pd.read_csv(dataset_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(12976, 28)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1 : Essays =  1783 \t Attributes =  6\n",
      "Set 2 : Essays =  1800 \t Attributes =  9\n",
      "Set 3 : Essays =  1726 \t Attributes =  6\n",
      "Set 4 : Essays =  1770 \t Attributes =  6\n",
      "Set 5 : Essays =  1805 \t Attributes =  6\n",
      "Set 6 : Essays =  1800 \t Attributes =  6\n",
      "Set 7 : Essays =  1569 \t Attributes =  14\n",
      "Set 8 : Essays =  723 \t Attributes =  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/2675578851.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set1.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set2.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set3.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set4.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set5.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set6.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set7.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "/var/folders/pj/ldkmdmbj5bld43hhctfjwh280000gn/T/ipykernel_6368/1243294380.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set8.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scores = [2, 1, 0, 0, 0, 0, 0, 0]\n",
    "max_scores = [12, 6, 3, 3, 4, 4, 30, 60]\n",
    "essay_sets, data_min_scores, data_max_scores = split_in_sets(data)\n",
    "set1, set2, set3, set4, set5, set6, set7, set8 = tuple(essay_sets)\n",
    "data.dropna(axis=1, inplace=True)\n",
    "data.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set1.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set2.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set3.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set4.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set5.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set6.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set7.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set8.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "sets = [set1,set2,set3,set4,set5,set6,set7,set8]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRZkhVx5txSk"
   },
   "outputs": [],
   "source": [
    "cap = ['@CAPS'+str(i) for i in range(100)]\n",
    "loc = ['@LOCATION'+str(i) for i in range(100)]\n",
    "org =['@ORGANIZATION'+str(i) for i in range(100)]\n",
    "per = ['@PERSON'+str(i) for i in range(100)]\n",
    "date = ['@DATE'+str(i) for i in range(100)]\n",
    "time = ['@TIME'+str(i) for i in range(100)]\n",
    "money = ['@MONEY'+str(i) for i in range(100)]\n",
    "ner =  cap + loc + org + per + date + time + money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Removing essays have less than 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>Being active has no limit, but technology does.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>545</td>\n",
       "      <td>1</td>\n",
       "      <td>I think that computers are amazing. Computers ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear readers, I think that its good and bad to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>6044</td>\n",
       "      <td>3</td>\n",
       "      <td>The setting affected the cyclist because of th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>6188</td>\n",
       "      <td>3</td>\n",
       "      <td>The features of the setting do affect the cycl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>18348</td>\n",
       "      <td>7</td>\n",
       "      <td>My dad can be very patient sometimes. When he...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>18958</td>\n",
       "      <td>7</td>\n",
       "      <td>I show loyalty a lot of peolpe that I no and t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11748</th>\n",
       "      <td>19010</td>\n",
       "      <td>7</td>\n",
       "      <td>I do not have patience.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12150</th>\n",
       "      <td>19450</td>\n",
       "      <td>7</td>\n",
       "      <td>I was patient when I was at the @CAPS1.R whe I...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12966</th>\n",
       "      <td>21619</td>\n",
       "      <td>8</td>\n",
       "      <td>I dont like computers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "356         359          1    Being active has no limit, but technology does.   \n",
       "542         545          1  I think that computers are amazing. Computers ...   \n",
       "1781       1786          1  Dear readers, I think that its good and bad to...   \n",
       "3648       6044          3  The setting affected the cyclist because of th...   \n",
       "3792       6188          3  The features of the setting do affect the cycl...   \n",
       "...         ...        ...                                                ...   \n",
       "11144     18348          7  My dad can be very patient sometimes. When he...   \n",
       "11699     18958          7  I show loyalty a lot of peolpe that I no and t...   \n",
       "11748     19010          7                            I do not have patience.   \n",
       "12150     19450          7  I was patient when I was at the @CAPS1.R whe I...   \n",
       "12966     21619          8                              I dont like computers   \n",
       "\n",
       "       domain1_score  \n",
       "356                2  \n",
       "542                2  \n",
       "1781               2  \n",
       "3648               2  \n",
       "3792               0  \n",
       "...              ...  \n",
       "11144              5  \n",
       "11699              2  \n",
       "11748              3  \n",
       "12150              4  \n",
       "12966             10  \n",
       "\n",
       "[87 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_less_100_words = data[data.essay.str.len() < 100]\n",
    "essays_less_100_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0       1.0        1.0  Dear local newspaper, I think effects computer...   \n",
       "1       2.0        1.0  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2       3.0        1.0  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3       4.0        1.0  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4       5.0        1.0  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0            8.0  \n",
       "1            9.0  \n",
       "2            7.0  \n",
       "3           10.0  \n",
       "4            8.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[~data.isin(essays_less_100_words)].dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12889, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalize score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each set has different score range, so we are going to normalize them to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m topic_number \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 2\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m2\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m7\u001B[39m), sharey\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(7, 7), sharey=False)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        data[data['essay_set'] == topic_number].groupby('domain1_score')['essay_id'].agg('count').plot.bar(ax=ax[i, j], rot=0, color=\"maroon\")\n",
    "        ax[i,j].set_title('Essay %i' % topic_number)\n",
    "\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       essay_id  essay_set                                              essay  \\\n0             1          1  Dear local newspaper, I think effects computer...   \n1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4             5          1  Dear @LOCATION1, I know having computers has a...   \n...         ...        ...                                                ...   \n12971     21626          8   In most stories mothers and daughters are eit...   \n12972     21628          8   I never understood the meaning laughter is th...   \n12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n12974     21630          8                                 Trippin' on fen...   \n12975     21633          8   Many people believe that laughter can improve...   \n\n       domain1_score  \n0               0.60  \n1               0.70  \n2               0.50  \n3               0.80  \n4               0.60  \n...              ...  \n12971           0.50  \n12972           0.44  \n12973           0.60  \n12974           0.60  \n12975           0.60  \n\n[12976 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12971</th>\n      <td>21626</td>\n      <td>8</td>\n      <td>In most stories mothers and daughters are eit...</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>12972</th>\n      <td>21628</td>\n      <td>8</td>\n      <td>I never understood the meaning laughter is th...</td>\n      <td>0.44</td>\n    </tr>\n    <tr>\n      <th>12973</th>\n      <td>21629</td>\n      <td>8</td>\n      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>12974</th>\n      <td>21630</td>\n      <td>8</td>\n      <td>Trippin' on fen...</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>12975</th>\n      <td>21633</td>\n      <td>8</td>\n      <td>Many people believe that laughter can improve...</td>\n      <td>0.60</td>\n    </tr>\n  </tbody>\n</table>\n<p>12976 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for topic in range(1, 9):\n",
    "    topic_set = data[data['essay_set'] == topic]\n",
    "    min_set = topic_set['domain1_score'].min()\n",
    "    max_set = topic_set['domain1_score'].max()\n",
    "\n",
    "    data.loc[data['essay_set'] == topic, 'domain1_score'] = data[data['essay_set'] == topic]['domain1_score'].apply(lambda x: (x - min_set) / (max_set - min_set))\n",
    "    # data[data['essay_set'] == topic]['domain1_score'].apply(lambda x: (x - min_set) / (max_set - min_set))\n",
    "    # data[data['essay_set'] == topic]['domain1_score']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m topic_number \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 2\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m2\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m7\u001B[39m), sharey\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(7, 7), sharey=False)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        data[data['essay_set'] == topic_number].groupby('domain1_score')['essay_id'].agg('count').plot.bar(ax=ax[i, j], rot=0, color=\"maroon\")\n",
    "        ax[i,j].set_title('Essay %i' % topic_number)\n",
    "\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional, Conv2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential,Model, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "w8pjFmyC8V6k",
    "outputId": "c1b34836-aa9d-41d4-9219-5e23bf14ff20"
   },
   "outputs": [],
   "source": [
    "def get_model(Hidden_dim1=400, Hidden_dim2=128, return_sequences = True, dropout=0.5, recurrent_dropout=0.4, input_size=768, activation='relu', bidirectional = False):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    if bidirectional:\n",
    "        model.add(Bidirectional(LSTM(Hidden_dim1,return_sequences=return_sequences , dropout=0.4, recurrent_dropout=recurrent_dropout), input_shape=[1, input_size]))\n",
    "        model.add(Bidirectional(LSTM(Hidden_dim2, recurrent_dropout=recurrent_dropout)))\n",
    "    else:\n",
    "        model.add(LSTM(Hidden_dim1, dropout=0.4, recurrent_dropout=recurrent_dropout, input_shape=[1, input_size], return_sequences=return_sequences))\n",
    "        model.add(LSTM(Hidden_dim2, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mse'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gW9U6fU5HTBW"
   },
   "source": [
    "## All sets using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(X):\n",
    "    max_len = 0\n",
    "    for i in X.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded_X = np.array([i + [0]*(max_len-len(i)) for i in X.values])\n",
    "    attention_mask_X = np.where(padded_X != 0, 1, 0)\n",
    "    X_input_ids = torch.tensor(padded_X)\n",
    "    X_attention_mask = torch.tensor(attention_mask_X)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states_X = model(X_input_ids, attention_mask=X_attention_mask)\n",
    "\n",
    "    X_features = last_hidden_states_X[0][:,0,:].numpy()\n",
    "\n",
    "    return X_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2654def4051f4fc58e7d2fb8c314365a",
      "02f24c9a529f453ea521216a0009cca0",
      "2e690d7a41c743a09bbd7b9e3dcbff57",
      "acb0089eba5a41d3a2fc5c3186bda706",
      "79a30aa163d943fa9c99be3ba26e4eed",
      "3c2db7a3d29748bb95a91095805a5390",
      "0779784dd6bc4ac4b86bdb1a0681733c",
      "76593de64e9c4d87bfa2946e82fe5193",
      "d6a67032d4cb46c88d0058bb7d3fe398",
      "f2fbe75c769f43c28265687fdf2fe9ad",
      "e695a2f265bd496b9ad8e3afcecd1ea8",
      "32f1b625e39349cbbcb9ed38ea3fb2ae",
      "8adca1280df0497fa40c4b90093982f9",
      "e6a72387356b4133ba9c1bf29f6bf368",
      "b376e92a82ff4658b1bcf53a712ceaef",
      "926f21ae08c544629beb194d9a278fc3",
      "1a5c8ebdcd884286bfe2bbdc739bab4e",
      "237351b8013e465f8e8edfab0b40f81c",
      "f100e48f3a1144c798acfb2ae4d546d2",
      "8a6eb9f25c6b4fc38423b696bcf63862",
      "a5b70a5b43ae4e7092e5449279fe6680",
      "82c96ecfea1d41769acbdf18468cb08e",
      "7f6636125e0749e0943e49e8b63e8852",
      "a20a8ee8e1ba4eee850775943c1d8d15"
     ]
    },
    "colab_type": "code",
    "id": "T1Aue2g3-ykA",
    "outputId": "e7469de9-7100-4755-9b9f-0538b2ac922b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------SET 1--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d85c1670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d85c1670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:04:34.480530: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 118ms/step - loss: 17.1625 - mse: 17.1625\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 2.6606 - mse: 2.6606\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2.7873 - mse: 2.7873\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2.3790 - mse: 2.3790\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.3948 - mse: 2.3948\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 2.2457 - mse: 2.2457\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.9842 - mse: 1.9842\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.1276 - mse: 2.1276\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.1237 - mse: 2.1237\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.2012 - mse: 2.2012\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.9474 - mse: 1.9474\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.0375 - mse: 2.0375\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.9145 - mse: 1.9145\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.9286 - mse: 1.9286\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.7987 - mse: 1.7987\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4897 - mse: 1.4897\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.8795 - mse: 1.8795\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6851 - mse: 1.6851\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6119 - mse: 1.6119\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.6780 - mse: 1.6780\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.4685 - mse: 1.4685\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 1.5995 - mse: 1.5995\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5952 - mse: 1.5952\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3160 - mse: 1.3160\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5146 - mse: 1.5146\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.7810 - mse: 1.7810\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2873 - mse: 1.2873\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.4900 - mse: 1.4900\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4744 - mse: 1.4744\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.5038 - mse: 1.5038\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5494 - mse: 1.5494\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4155 - mse: 1.4155\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3716 - mse: 1.3716\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2325 - mse: 1.2325\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5229 - mse: 1.5229\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2118 - mse: 1.2118\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.4432 - mse: 1.4432\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2969 - mse: 1.2969\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3258 - mse: 1.3258\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.5716 - mse: 1.5716\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2626 - mse: 1.2626\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3522 - mse: 1.3522\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2261 - mse: 1.2261\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3429 - mse: 1.3429\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2870 - mse: 1.2870\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2239 - mse: 1.2239\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3747 - mse: 1.3747\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2557 - mse: 1.2557\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2090 - mse: 1.2090\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3345 - mse: 1.3345\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2662 - mse: 1.2662\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2546 - mse: 1.2546\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2897 - mse: 1.2897\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3296 - mse: 1.3296\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.1435 - mse: 1.1435\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.3088 - mse: 1.3088\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.2029 - mse: 1.2029\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2522 - mse: 1.2522\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.1859 - mse: 1.1859\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3317 - mse: 1.3317\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.0772 - mse: 1.0772\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2751 - mse: 1.2751\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.1777 - mse: 1.1777\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.1781 - mse: 1.1781\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2237 - mse: 1.2237\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.1356 - mse: 1.1356\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2129 - mse: 1.2129\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.1903 - mse: 1.1903\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2277 - mse: 1.2277\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.0844 - mse: 1.0844\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16bb8aca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16bb8aca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:05:34.610949: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.4902322736503508\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x305637a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x305637a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:06:55.707729: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 137ms/step - loss: 18.3049 - mse: 18.3049\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 3.3178 - mse: 3.3178\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.0481 - mse: 3.0481\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 2.6909 - mse: 2.6909\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.3761 - mse: 2.3761\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.3277 - mse: 2.3277\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 2.2957 - mse: 2.2957\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.4141 - mse: 2.4141\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.9449 - mse: 1.9449\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2.0908 - mse: 2.0908\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.1084 - mse: 2.1084\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.0521 - mse: 2.0521\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.5991 - mse: 1.5991\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.9433 - mse: 1.9433\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.0215 - mse: 2.0215\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6958 - mse: 1.6958\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6491 - mse: 1.6491\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.8120 - mse: 1.8120\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.7532 - mse: 1.7532\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.7234 - mse: 1.7234\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6734 - mse: 1.6734\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.5685 - mse: 1.5685\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.7736 - mse: 1.7736\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6002 - mse: 1.6002\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.6153 - mse: 1.6153\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5586 - mse: 1.5586\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.6063 - mse: 1.6063\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.6513 - mse: 1.6513\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4609 - mse: 1.4609\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6155 - mse: 1.6155\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.4850 - mse: 1.4850\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3643 - mse: 1.3643\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5416 - mse: 1.5416\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.2608 - mse: 1.2608\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.6066 - mse: 1.6066\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3635 - mse: 1.3635\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.4271 - mse: 1.4271\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3610 - mse: 1.3610\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.5312 - mse: 1.5312\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3538 - mse: 1.3538\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.4469 - mse: 1.4469\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.3512 - mse: 1.3512\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.4738 - mse: 1.4738\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4782 - mse: 1.4782\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.3326 - mse: 1.3326\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3700 - mse: 1.3700\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.1463 - mse: 1.1463\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6042 - mse: 1.6042\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.1764 - mse: 1.1764\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.3872 - mse: 1.3872\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 1.3313 - mse: 1.3313\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.2484 - mse: 1.2484\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.2492 - mse: 1.2492\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 1.2656 - mse: 1.2656\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.3528 - mse: 1.3528\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.1445 - mse: 1.1445\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1.3680 - mse: 1.3680\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.1764 - mse: 1.1764\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.2910 - mse: 1.2910\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.3735 - mse: 1.3735\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.2088 - mse: 1.2088\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.1403 - mse: 1.1403\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2509 - mse: 1.2509\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1.2020 - mse: 1.2020\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.3985 - mse: 1.3985\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.1703 - mse: 1.1703\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.2014 - mse: 1.2014\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.2258 - mse: 1.2258\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.2291 - mse: 1.2291\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1.2495 - mse: 1.2495\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d52aa3a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d52aa3a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:07:57.987932: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6140038533765821\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b1b5c310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b1b5c310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:09:20.509333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 140ms/step - loss: 16.5596 - mse: 16.5596\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.4520 - mse: 3.4520\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.2439 - mse: 3.2439\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.5710 - mse: 2.5710\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.3298 - mse: 2.3298\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.1229 - mse: 2.1229\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.5163 - mse: 2.5163\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 2.3288 - mse: 2.3288\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.0018 - mse: 2.0018\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.9029 - mse: 1.9029\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.1890 - mse: 2.1890\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.0081 - mse: 2.0081\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 1.9572 - mse: 1.9572\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.8442 - mse: 1.8442\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.8822 - mse: 1.8822\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.8412 - mse: 1.8412\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.7784 - mse: 1.7784\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.7674 - mse: 1.7674\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.4854 - mse: 1.4854\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.7336 - mse: 1.7336\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.6847 - mse: 1.6847\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5331 - mse: 1.5331\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.6806 - mse: 1.6806\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5000 - mse: 1.5000\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.6636 - mse: 1.6636\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.6313 - mse: 1.6313\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.5679 - mse: 1.5679\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.5288 - mse: 1.5288\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.6302 - mse: 1.6302\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5107 - mse: 1.5107\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.4361 - mse: 1.4361\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.4721 - mse: 1.4721\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5495 - mse: 1.5495\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5024 - mse: 1.5024\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4467 - mse: 1.4467\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3564 - mse: 1.3564\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.4979 - mse: 1.4979\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4437 - mse: 1.4437\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4326 - mse: 1.4326\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3232 - mse: 1.3232\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3835 - mse: 1.3835\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4454 - mse: 1.4454\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.4289 - mse: 1.4289\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.3441 - mse: 1.3441\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3453 - mse: 1.3453\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3675 - mse: 1.3675\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.5554 - mse: 1.5554\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.1825 - mse: 1.1825\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.3348 - mse: 1.3348\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3417 - mse: 1.3417\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3808 - mse: 1.3808\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.2355 - mse: 1.2355\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3386 - mse: 1.3386\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.2805 - mse: 1.2805\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.3173 - mse: 1.3173\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.1937 - mse: 1.1937\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.3213 - mse: 1.3213\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.1744 - mse: 1.1744\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.4282 - mse: 1.4282\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2363 - mse: 1.2363\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2163 - mse: 1.2163\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2611 - mse: 1.2611\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2652 - mse: 1.2652\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.2899 - mse: 1.2899\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2595 - mse: 1.2595\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.1925 - mse: 1.1925\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2700 - mse: 1.2700\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.2058 - mse: 1.2058\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.1655 - mse: 1.1655\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.1681 - mse: 1.1681\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2bc7a49d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2bc7a49d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:10:20.499004: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5588189588189587\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2bd582430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2bd582430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:11:37.808934: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 158ms/step - loss: 18.6869 - mse: 18.6869\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 3.2230 - mse: 3.2230\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 2.9514 - mse: 2.9514\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 2.4686 - mse: 2.4686\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 2.3908 - mse: 2.3908\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.3325 - mse: 2.3325\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.2788 - mse: 2.2788\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2.3527 - mse: 2.3527\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 2.2054 - mse: 2.2054\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.0394 - mse: 2.0394\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 1.9304 - mse: 1.9304\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2.0253 - mse: 2.0253\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.8806 - mse: 1.8806\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.8070 - mse: 1.8070\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.7874 - mse: 1.7874\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.9249 - mse: 1.9249\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.8828 - mse: 1.8828\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 1.8846 - mse: 1.8846\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.8161 - mse: 1.8161\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 1.5996 - mse: 1.5996\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.8158 - mse: 1.8158\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4148 - mse: 1.4148\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.8970 - mse: 1.8970\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 1.6337 - mse: 1.6337\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 1.5717 - mse: 1.5717\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.5402 - mse: 1.5402\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1.6645 - mse: 1.6645\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1.5475 - mse: 1.5475\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 1.6551 - mse: 1.6551\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1.4546 - mse: 1.4546\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4504 - mse: 1.4504\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.5258 - mse: 1.5258\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.4687 - mse: 1.4687\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.5140 - mse: 1.5140\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.5260 - mse: 1.5260\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 1.4232 - mse: 1.4232\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.4889 - mse: 1.4889\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.4020 - mse: 1.4020\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.2587 - mse: 1.2587\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1.5466 - mse: 1.5466\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3088 - mse: 1.3088\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.4757 - mse: 1.4757\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.3729 - mse: 1.3729\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3867 - mse: 1.3867\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3473 - mse: 1.3473\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.4449 - mse: 1.4449\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3481 - mse: 1.3481\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.5102 - mse: 1.5102\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2199 - mse: 1.2199\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.4803 - mse: 1.4803\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2077 - mse: 1.2077\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 1.3790 - mse: 1.3790\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.2750 - mse: 1.2750\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3306 - mse: 1.3306\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.1683 - mse: 1.1683\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3855 - mse: 1.3855\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.1972 - mse: 1.1972\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.3589 - mse: 1.3589\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.2651 - mse: 1.2651\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3621 - mse: 1.3621\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.1801 - mse: 1.1801\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3044 - mse: 1.3044\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.4840 - mse: 1.4840\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.2196 - mse: 1.2196\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.0938 - mse: 1.0938\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3217 - mse: 1.3217\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.0772 - mse: 1.0772\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3828 - mse: 1.3828\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 1.1957 - mse: 1.1957\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.3405 - mse: 1.3405\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16a42c5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16a42c5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:12:38.626548: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.49169946438611545\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b4ebb430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b4ebb430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:14:03.561468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 151ms/step - loss: 23.9137 - mse: 23.9137\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 2.9307 - mse: 2.9307\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.7474 - mse: 2.7474\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 2.6420 - mse: 2.6420\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 2.4353 - mse: 2.4353\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 2.3740 - mse: 2.3740\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 2.1096 - mse: 2.1096\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 2.2128 - mse: 2.2128\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 2.2328 - mse: 2.2328\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.8211 - mse: 1.8211\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.9505 - mse: 1.9505\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.8768 - mse: 1.8768\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.7816 - mse: 1.7816\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 2.0205 - mse: 2.0205\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.7887 - mse: 1.7887\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.7146 - mse: 1.7146\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.6454 - mse: 1.6454\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.8272 - mse: 1.8272\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.6802 - mse: 1.6802\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.6113 - mse: 1.6113\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.6708 - mse: 1.6708\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.6200 - mse: 1.6200\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.6087 - mse: 1.6087\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.5973 - mse: 1.5973\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.5993 - mse: 1.5993\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.4185 - mse: 1.4185\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.5665 - mse: 1.5665\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.5293 - mse: 1.5293\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.5943 - mse: 1.5943\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.4879 - mse: 1.4879\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.2915 - mse: 1.2915\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 1.5205 - mse: 1.5205\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.4254 - mse: 1.4254\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.4937 - mse: 1.4937\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.3932 - mse: 1.3932\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.4113 - mse: 1.4113\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.3288 - mse: 1.3288\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.5174 - mse: 1.5174\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 1.3200 - mse: 1.3200\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.5453 - mse: 1.5453\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.2586 - mse: 1.2586\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.4283 - mse: 1.4283\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.2648 - mse: 1.2648\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.4791 - mse: 1.4791\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.3930 - mse: 1.3930\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.4394 - mse: 1.4394\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.2649 - mse: 1.2649\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.3048 - mse: 1.3048\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.3195 - mse: 1.3195\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 1.3172 - mse: 1.3172\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.3356 - mse: 1.3356\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.3266 - mse: 1.3266\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.4902 - mse: 1.4902\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.2093 - mse: 1.2093\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.2972 - mse: 1.2972\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.2736 - mse: 1.2736\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.2646 - mse: 1.2646\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 1.4016 - mse: 1.4016\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.0845 - mse: 1.0845\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.3539 - mse: 1.3539\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.2637 - mse: 1.2637\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.2081 - mse: 1.2081\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.1636 - mse: 1.1636\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.2728 - mse: 1.2728\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.2487 - mse: 1.2487\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 1.2514 - mse: 1.2514\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 1.2416 - mse: 1.2416\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.2142 - mse: 1.2142\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 1.1834 - mse: 1.1834\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.1901 - mse: 1.1901\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16a42c160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x16a42c160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:14:58.454430: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5630203964730672\n",
      "Average kappa score value is : 0.5435549893410149\n",
      "\n",
      "--------SET 2--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa727af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa727af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:16:34.741253: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 153ms/step - loss: 11.5714 - mse: 11.5714\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.9074 - mse: 0.9074\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.7820 - mse: 0.7820\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5331 - mse: 0.5331\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5815 - mse: 0.5815\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.6188 - mse: 0.6188\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4609 - mse: 0.4609\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5286 - mse: 0.5286\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5480 - mse: 0.5480\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5343 - mse: 0.5343\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4325 - mse: 0.4325\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4693 - mse: 0.4693\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4636 - mse: 0.4636\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4323 - mse: 0.4323\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4863 - mse: 0.4863\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4280 - mse: 0.4280\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4554 - mse: 0.4554\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4254 - mse: 0.4254\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4442 - mse: 0.4442\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4416 - mse: 0.4416\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4246 - mse: 0.4246\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3772 - mse: 0.3772\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4354 - mse: 0.4354\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3826 - mse: 0.3826\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4228 - mse: 0.4228\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3797 - mse: 0.3797\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4050 - mse: 0.4050\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3915 - mse: 0.3915\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3794 - mse: 0.3794\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4080 - mse: 0.4080\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3924 - mse: 0.3924\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3932 - mse: 0.3932\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3561 - mse: 0.3561\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3834 - mse: 0.3834\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3678 - mse: 0.3678\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3806 - mse: 0.3806\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3630 - mse: 0.3630\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3565 - mse: 0.3565\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3911 - mse: 0.3911\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3644 - mse: 0.3644\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3627 - mse: 0.3627\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3539 - mse: 0.3539\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3365 - mse: 0.3365\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3557 - mse: 0.3557\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3542 - mse: 0.3542\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3663 - mse: 0.3663\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3553 - mse: 0.3553\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3430 - mse: 0.3430\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3501 - mse: 0.3501\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3182 - mse: 0.3182\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3594 - mse: 0.3594\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3448 - mse: 0.3448\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3646 - mse: 0.3646\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3278 - mse: 0.3278\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3312 - mse: 0.3312\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3315 - mse: 0.3315\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3399 - mse: 0.3399\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3343 - mse: 0.3343\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3426 - mse: 0.3426\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3298 - mse: 0.3298\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3243 - mse: 0.3243\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3200 - mse: 0.3200\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3497 - mse: 0.3497\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3202 - mse: 0.3202\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3310 - mse: 0.3310\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3094 - mse: 0.3094\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3312 - mse: 0.3312\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2936 - mse: 0.2936\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3184 - mse: 0.3184\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x305b1b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x305b1b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:17:32.425088: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.4739756367663345\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16af70040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16af70040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:18:54.098057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 151ms/step - loss: 2.6037 - mse: 2.6037\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.7789 - mse: 0.7789\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.7124 - mse: 0.7124\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.6688 - mse: 0.6688\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4873 - mse: 0.4873\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5139 - mse: 0.5139\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.6165 - mse: 0.6165\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4881 - mse: 0.4881\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.5322 - mse: 0.5322\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4765 - mse: 0.4765\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4665 - mse: 0.4665\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4939 - mse: 0.4939\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.5141 - mse: 0.5141\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4424 - mse: 0.4424\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4751 - mse: 0.4751\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4497 - mse: 0.4497\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4633 - mse: 0.4633\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4486 - mse: 0.4486\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4261 - mse: 0.4261\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4512 - mse: 0.4512\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4460 - mse: 0.4460\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4033 - mse: 0.4033\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3910 - mse: 0.3910\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4045 - mse: 0.4045\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4323 - mse: 0.4323\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4149 - mse: 0.4149\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3944 - mse: 0.3944\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4079 - mse: 0.4079\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.3910 - mse: 0.3910\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.4417 - mse: 0.4417\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3850 - mse: 0.3850\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3740 - mse: 0.3740\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.3964 - mse: 0.3964\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3609 - mse: 0.3609\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3899 - mse: 0.3899\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3661 - mse: 0.3661\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3714 - mse: 0.3714\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4034 - mse: 0.4034\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3586 - mse: 0.3586\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3420 - mse: 0.3420\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3966 - mse: 0.3966\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3426 - mse: 0.3426\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3805 - mse: 0.3805\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3533 - mse: 0.3533\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3706 - mse: 0.3706\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.4002 - mse: 0.4002\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3214 - mse: 0.3214\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3423 - mse: 0.3423\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3642 - mse: 0.3642\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3942 - mse: 0.3942\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3400 - mse: 0.3400\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3707 - mse: 0.3707\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3602 - mse: 0.3602\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3466 - mse: 0.3466\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3358 - mse: 0.3358\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3386 - mse: 0.3386\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3535 - mse: 0.3535\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3113 - mse: 0.3113\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3333 - mse: 0.3333\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3489 - mse: 0.3489\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3208 - mse: 0.3208\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3518 - mse: 0.3518\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3235 - mse: 0.3235\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3162 - mse: 0.3162\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3339 - mse: 0.3339\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3252 - mse: 0.3252\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3368 - mse: 0.3368\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3481 - mse: 0.3481\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.2950 - mse: 0.2950\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3271 - mse: 0.3271\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2f62dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2f62dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:19:51.198214: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5209580838323353\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5617430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5617430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:21:17.219527: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 156ms/step - loss: 2.8829 - mse: 2.8829\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.6186 - mse: 0.6186\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.6456 - mse: 0.6456\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5208 - mse: 0.5208\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5953 - mse: 0.5953\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5098 - mse: 0.5098\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5122 - mse: 0.5122\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4836 - mse: 0.4836\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5138 - mse: 0.5138\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4759 - mse: 0.4759\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4576 - mse: 0.4576\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4465 - mse: 0.4465\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4769 - mse: 0.4769\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4176 - mse: 0.4176\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4387 - mse: 0.4387\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4510 - mse: 0.4510\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3924 - mse: 0.3924\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4069 - mse: 0.4069\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4183 - mse: 0.4183\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4197 - mse: 0.4197\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3923 - mse: 0.3923\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4216 - mse: 0.4216\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3747 - mse: 0.3747\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4078 - mse: 0.4078\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3804 - mse: 0.3804\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3569 - mse: 0.3569\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4068 - mse: 0.4068\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3761 - mse: 0.3761\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3784 - mse: 0.3784\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3677 - mse: 0.3677\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3777 - mse: 0.3777\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3751 - mse: 0.3751\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3702 - mse: 0.3702\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3602 - mse: 0.3602\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3618 - mse: 0.3618\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3502 - mse: 0.3502\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3629 - mse: 0.3629\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3386 - mse: 0.3386\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3362 - mse: 0.3362\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3746 - mse: 0.3746\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3263 - mse: 0.3263\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3426 - mse: 0.3426\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3467 - mse: 0.3467\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3431 - mse: 0.3431\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3567 - mse: 0.3567\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3114 - mse: 0.3114\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3322 - mse: 0.3322\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3430 - mse: 0.3430\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3139 - mse: 0.3139\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3418 - mse: 0.3418\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3416 - mse: 0.3416\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3314 - mse: 0.3314\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3329 - mse: 0.3329\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3133 - mse: 0.3133\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3176 - mse: 0.3176\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3157 - mse: 0.3157\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3345 - mse: 0.3345\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3130 - mse: 0.3130\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3154 - mse: 0.3154\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3063 - mse: 0.3063\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3317 - mse: 0.3317\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2872 - mse: 0.2872\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3128 - mse: 0.3128\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2989 - mse: 0.2989\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3068 - mse: 0.3068\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3106 - mse: 0.3106\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3000 - mse: 0.3000\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3094 - mse: 0.3094\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2934 - mse: 0.2934\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b565ea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b565ea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:22:13.962029: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.551441022801146\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e59f94c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e59f94c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:23:36.519389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 155ms/step - loss: 3.0474 - mse: 3.0474\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.6337 - mse: 0.6337\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.6428 - mse: 0.6428\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.6622 - mse: 0.6622\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.7097 - mse: 0.7097\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5737 - mse: 0.5737\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5380 - mse: 0.5380\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4840 - mse: 0.4840\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5839 - mse: 0.5839\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5440 - mse: 0.5440\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4795 - mse: 0.4795\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4168 - mse: 0.4168\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4791 - mse: 0.4791\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5082 - mse: 0.5082\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5050 - mse: 0.5050\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4145 - mse: 0.4145\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4738 - mse: 0.4738\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4372 - mse: 0.4372\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4480 - mse: 0.4480\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4175 - mse: 0.4175\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4152 - mse: 0.4152\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4713 - mse: 0.4713\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4169 - mse: 0.4169\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4398 - mse: 0.4398\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4086 - mse: 0.4086\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3846 - mse: 0.3846\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3871 - mse: 0.3871\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4211 - mse: 0.4211\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3556 - mse: 0.3556\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4094 - mse: 0.4094\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4315 - mse: 0.4315\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3597 - mse: 0.3597\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3894 - mse: 0.3894\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3970 - mse: 0.3970\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3888 - mse: 0.3888\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3823 - mse: 0.3823\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3825 - mse: 0.3825\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3667 - mse: 0.3667\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3681 - mse: 0.3681\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3787 - mse: 0.3787\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3650 - mse: 0.3650\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3453 - mse: 0.3453\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4039 - mse: 0.4039\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3384 - mse: 0.3384\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3810 - mse: 0.3810\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3641 - mse: 0.3641\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3587 - mse: 0.3587\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3614 - mse: 0.3614\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3300 - mse: 0.3300\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3885 - mse: 0.3885\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3677 - mse: 0.3677\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3391 - mse: 0.3391\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3551 - mse: 0.3551\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3217 - mse: 0.3217\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3655 - mse: 0.3655\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3537 - mse: 0.3537\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3424 - mse: 0.3424\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3727 - mse: 0.3727\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3223 - mse: 0.3223\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3542 - mse: 0.3542\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3420 - mse: 0.3420\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3671 - mse: 0.3671\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3214 - mse: 0.3214\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3350 - mse: 0.3350\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3060 - mse: 0.3060\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3204 - mse: 0.3204\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3529 - mse: 0.3529\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3178 - mse: 0.3178\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3222 - mse: 0.3222\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b22bb3a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b22bb3a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:24:34.335310: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5145972564192753\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b4b51790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b4b51790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:25:59.127005: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 159ms/step - loss: 2.7369 - mse: 2.7369\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.7810 - mse: 0.7810\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.6856 - mse: 0.6856\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5952 - mse: 0.5952\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.6786 - mse: 0.6786\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5774 - mse: 0.5774\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5162 - mse: 0.5162\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5283 - mse: 0.5283\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5477 - mse: 0.5477\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4857 - mse: 0.4857\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5198 - mse: 0.5198\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5141 - mse: 0.5141\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5277 - mse: 0.5277\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4382 - mse: 0.4382\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4889 - mse: 0.4889\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4792 - mse: 0.4792\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4559 - mse: 0.4559\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4622 - mse: 0.4622\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4147 - mse: 0.4147\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4598 - mse: 0.4598\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4401 - mse: 0.4401\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4811 - mse: 0.4811\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3880 - mse: 0.3880\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4296 - mse: 0.4296\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4500 - mse: 0.4500\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4049 - mse: 0.4049\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3904 - mse: 0.3904\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4038 - mse: 0.4038\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4018 - mse: 0.4018\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4031 - mse: 0.4031\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3963 - mse: 0.3963\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4158 - mse: 0.4158\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3752 - mse: 0.3752\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4237 - mse: 0.4237\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3897 - mse: 0.3897\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3999 - mse: 0.3999\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3757 - mse: 0.3757\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3740 - mse: 0.3740\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3587 - mse: 0.3587\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3836 - mse: 0.3836\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3822 - mse: 0.3822\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3845 - mse: 0.3845\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4048 - mse: 0.4048\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3693 - mse: 0.3693\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3724 - mse: 0.3724\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3735 - mse: 0.3735\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3695 - mse: 0.3695\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3468 - mse: 0.3468\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3927 - mse: 0.3927\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3520 - mse: 0.3520\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3813 - mse: 0.3813\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3755 - mse: 0.3755\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3421 - mse: 0.3421\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3709 - mse: 0.3709\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3386 - mse: 0.3386\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3776 - mse: 0.3776\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3303 - mse: 0.3303\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3311 - mse: 0.3311\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3621 - mse: 0.3621\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3532 - mse: 0.3532\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3362 - mse: 0.3362\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3167 - mse: 0.3167\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3405 - mse: 0.3405\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3424 - mse: 0.3424\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3180 - mse: 0.3180\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3378 - mse: 0.3378\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3199 - mse: 0.3199\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3624 - mse: 0.3624\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3087 - mse: 0.3087\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e49d5ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e49d5ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:26:57.823658: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.4860094705122686\n",
      "Average kappa score value is : 0.5093962940662718\n",
      "\n",
      "--------SET 3--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab9079d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab9079d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:28:25.015902: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 165ms/step - loss: 1.3533 - mse: 1.3533\n",
      "Epoch 2/70\n",
      "11/11 [==============================] - 1s 89ms/step - loss: 0.6543 - mse: 0.6543\n",
      "Epoch 3/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.6353 - mse: 0.6353\n",
      "Epoch 4/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.5724 - mse: 0.5724\n",
      "Epoch 5/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.5683 - mse: 0.5683\n",
      "Epoch 6/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4830 - mse: 0.4830\n",
      "Epoch 7/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.5097 - mse: 0.5097\n",
      "Epoch 8/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4694 - mse: 0.4694\n",
      "Epoch 9/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4702 - mse: 0.4702\n",
      "Epoch 10/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.4984 - mse: 0.4984\n",
      "Epoch 11/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4867 - mse: 0.4867\n",
      "Epoch 12/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4479 - mse: 0.4479\n",
      "Epoch 13/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4586 - mse: 0.4586\n",
      "Epoch 14/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.4309 - mse: 0.4309\n",
      "Epoch 15/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 16/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4299 - mse: 0.4299\n",
      "Epoch 17/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3982 - mse: 0.3982\n",
      "Epoch 18/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4131 - mse: 0.4131\n",
      "Epoch 19/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3860 - mse: 0.3860\n",
      "Epoch 20/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4107 - mse: 0.4107\n",
      "Epoch 21/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3754 - mse: 0.3754\n",
      "Epoch 22/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.4036 - mse: 0.4036\n",
      "Epoch 23/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3851 - mse: 0.3851\n",
      "Epoch 24/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3766 - mse: 0.3766\n",
      "Epoch 25/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.3610 - mse: 0.3610\n",
      "Epoch 26/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3973 - mse: 0.3973\n",
      "Epoch 27/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3821 - mse: 0.3821\n",
      "Epoch 28/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3559 - mse: 0.3559\n",
      "Epoch 29/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3634 - mse: 0.3634\n",
      "Epoch 30/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3890 - mse: 0.3890\n",
      "Epoch 31/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3935 - mse: 0.3935\n",
      "Epoch 32/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3609 - mse: 0.3609\n",
      "Epoch 33/70\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.3676 - mse: 0.3676\n",
      "Epoch 34/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.3114 - mse: 0.3114\n",
      "Epoch 35/70\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.3726 - mse: 0.3726\n",
      "Epoch 36/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3458 - mse: 0.3458\n",
      "Epoch 37/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3302 - mse: 0.3302\n",
      "Epoch 38/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3632 - mse: 0.3632\n",
      "Epoch 39/70\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.3325 - mse: 0.3325\n",
      "Epoch 40/70\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.3490 - mse: 0.3490\n",
      "Epoch 41/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3643 - mse: 0.3643\n",
      "Epoch 42/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.3244 - mse: 0.3244\n",
      "Epoch 43/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3177 - mse: 0.3177\n",
      "Epoch 44/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.3350 - mse: 0.3350\n",
      "Epoch 45/70\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.3397 - mse: 0.3397\n",
      "Epoch 46/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3371 - mse: 0.3371\n",
      "Epoch 47/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3131 - mse: 0.3131\n",
      "Epoch 48/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3364 - mse: 0.3364\n",
      "Epoch 49/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.2791 - mse: 0.2791\n",
      "Epoch 50/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.3410 - mse: 0.3410\n",
      "Epoch 51/70\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 52/70\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.3118 - mse: 0.3118\n",
      "Epoch 53/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3160 - mse: 0.3160\n",
      "Epoch 54/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.2865 - mse: 0.2865\n",
      "Epoch 55/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3163 - mse: 0.3163\n",
      "Epoch 56/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.2902 - mse: 0.2902\n",
      "Epoch 57/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.3134 - mse: 0.3134\n",
      "Epoch 58/70\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.3073 - mse: 0.3073\n",
      "Epoch 59/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.2838 - mse: 0.2838\n",
      "Epoch 60/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.2927 - mse: 0.2927\n",
      "Epoch 61/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.2793 - mse: 0.2793\n",
      "Epoch 62/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3088 - mse: 0.3088\n",
      "Epoch 63/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.2831 - mse: 0.2831\n",
      "Epoch 64/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.2914 - mse: 0.2914\n",
      "Epoch 65/70\n",
      "11/11 [==============================] - 1s 99ms/step - loss: 0.2660 - mse: 0.2660\n",
      "Epoch 66/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.3062 - mse: 0.3062\n",
      "Epoch 67/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.2886 - mse: 0.2886\n",
      "Epoch 68/70\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.2855 - mse: 0.2855\n",
      "Epoch 69/70\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.2563 - mse: 0.2563\n",
      "Epoch 70/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.2710 - mse: 0.2710\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x17697d8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x17697d8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:29:23.367007: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5863404448785088\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16ae7fdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16ae7fdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:30:37.195884: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 162ms/step - loss: 1.3317 - mse: 1.3317\n",
      "Epoch 2/70\n",
      "11/11 [==============================] - 1s 92ms/step - loss: 0.6855 - mse: 0.6855\n",
      "Epoch 3/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.5917 - mse: 0.5917\n",
      "Epoch 4/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.5915 - mse: 0.5915\n",
      "Epoch 5/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.5328 - mse: 0.5328\n",
      "Epoch 6/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4734 - mse: 0.4734\n",
      "Epoch 7/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4714 - mse: 0.4714\n",
      "Epoch 8/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.4512 - mse: 0.4512\n",
      "Epoch 9/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.5234 - mse: 0.5234\n",
      "Epoch 10/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.4234 - mse: 0.4234\n",
      "Epoch 11/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4717 - mse: 0.4717\n",
      "Epoch 12/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4016 - mse: 0.4016\n",
      "Epoch 13/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4877 - mse: 0.4877\n",
      "Epoch 14/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4221 - mse: 0.4221\n",
      "Epoch 15/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4222 - mse: 0.4222\n",
      "Epoch 16/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.4261 - mse: 0.4261\n",
      "Epoch 17/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3993 - mse: 0.3993\n",
      "Epoch 18/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3845 - mse: 0.3845\n",
      "Epoch 19/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.4169 - mse: 0.4169\n",
      "Epoch 20/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3557 - mse: 0.3557\n",
      "Epoch 21/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3782 - mse: 0.3782\n",
      "Epoch 22/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.4162 - mse: 0.4162\n",
      "Epoch 23/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4042 - mse: 0.4042\n",
      "Epoch 24/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.3778 - mse: 0.3778\n",
      "Epoch 25/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3935 - mse: 0.3935\n",
      "Epoch 26/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3850 - mse: 0.3850\n",
      "Epoch 27/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3586 - mse: 0.3586\n",
      "Epoch 28/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3812 - mse: 0.3812\n",
      "Epoch 29/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3711 - mse: 0.3711\n",
      "Epoch 30/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3680 - mse: 0.3680\n",
      "Epoch 31/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3312 - mse: 0.3312\n",
      "Epoch 32/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3760 - mse: 0.3760\n",
      "Epoch 33/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3523 - mse: 0.3523\n",
      "Epoch 34/70\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.3666 - mse: 0.3666\n",
      "Epoch 35/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3402 - mse: 0.3402\n",
      "Epoch 36/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3416 - mse: 0.3416\n",
      "Epoch 37/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3165 - mse: 0.3165\n",
      "Epoch 38/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3676 - mse: 0.3676\n",
      "Epoch 39/70\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.3302 - mse: 0.3302\n",
      "Epoch 40/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3479 - mse: 0.3479\n",
      "Epoch 41/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3191 - mse: 0.3191\n",
      "Epoch 42/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3322 - mse: 0.3322\n",
      "Epoch 43/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3501 - mse: 0.3501\n",
      "Epoch 44/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3178 - mse: 0.3178\n",
      "Epoch 45/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3311 - mse: 0.3311\n",
      "Epoch 46/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3280 - mse: 0.3280\n",
      "Epoch 47/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3279 - mse: 0.3279\n",
      "Epoch 48/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3049 - mse: 0.3049\n",
      "Epoch 49/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3028 - mse: 0.3028\n",
      "Epoch 50/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3144 - mse: 0.3144\n",
      "Epoch 51/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3206 - mse: 0.3206\n",
      "Epoch 52/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.2781 - mse: 0.2781\n",
      "Epoch 53/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3563 - mse: 0.3563\n",
      "Epoch 54/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2933 - mse: 0.2933\n",
      "Epoch 55/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3084 - mse: 0.3084\n",
      "Epoch 56/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2832 - mse: 0.2832\n",
      "Epoch 57/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3075 - mse: 0.3075\n",
      "Epoch 58/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3094 - mse: 0.3094\n",
      "Epoch 59/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3054 - mse: 0.3054\n",
      "Epoch 60/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.2903 - mse: 0.2903\n",
      "Epoch 61/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3058 - mse: 0.3058\n",
      "Epoch 62/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2698 - mse: 0.2698\n",
      "Epoch 63/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3145 - mse: 0.3145\n",
      "Epoch 64/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.2901 - mse: 0.2901\n",
      "Epoch 65/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.2949 - mse: 0.2949\n",
      "Epoch 66/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3063 - mse: 0.3063\n",
      "Epoch 67/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2774 - mse: 0.2774\n",
      "Epoch 68/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2759 - mse: 0.2759\n",
      "Epoch 69/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2885 - mse: 0.2885\n",
      "Epoch 70/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2713 - mse: 0.2713\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2f130baf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2f130baf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:31:31.109984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6351334860954808\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5b16550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d5b16550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:32:45.925482: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 7s 163ms/step - loss: 1.5870 - mse: 1.5870\n",
      "Epoch 2/70\n",
      "11/11 [==============================] - 1s 87ms/step - loss: 0.5795 - mse: 0.5795\n",
      "Epoch 3/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.5635 - mse: 0.5635\n",
      "Epoch 4/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.6054 - mse: 0.6054\n",
      "Epoch 5/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.5508 - mse: 0.5508\n",
      "Epoch 6/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.4836 - mse: 0.4836\n",
      "Epoch 7/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.5347 - mse: 0.5347\n",
      "Epoch 8/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.4999 - mse: 0.4999\n",
      "Epoch 9/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4558 - mse: 0.4558\n",
      "Epoch 10/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4457 - mse: 0.4457\n",
      "Epoch 11/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.5023 - mse: 0.5023\n",
      "Epoch 12/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4380 - mse: 0.4380\n",
      "Epoch 13/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4694 - mse: 0.4694\n",
      "Epoch 14/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3810 - mse: 0.3810\n",
      "Epoch 15/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4449 - mse: 0.4449\n",
      "Epoch 16/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3918 - mse: 0.3918\n",
      "Epoch 17/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4427 - mse: 0.4427\n",
      "Epoch 18/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3753 - mse: 0.3753\n",
      "Epoch 19/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4056 - mse: 0.4056\n",
      "Epoch 20/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3431 - mse: 0.3431\n",
      "Epoch 21/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4028 - mse: 0.4028\n",
      "Epoch 22/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3991 - mse: 0.3991\n",
      "Epoch 23/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3243 - mse: 0.3243\n",
      "Epoch 24/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4196 - mse: 0.4196\n",
      "Epoch 25/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3567 - mse: 0.3567\n",
      "Epoch 26/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3790 - mse: 0.3790\n",
      "Epoch 27/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3410 - mse: 0.3410\n",
      "Epoch 28/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3637 - mse: 0.3637\n",
      "Epoch 29/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3417 - mse: 0.3417\n",
      "Epoch 30/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3569 - mse: 0.3569\n",
      "Epoch 31/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3419 - mse: 0.3419\n",
      "Epoch 32/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3740 - mse: 0.3740\n",
      "Epoch 33/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3410 - mse: 0.3410\n",
      "Epoch 34/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3101 - mse: 0.3101\n",
      "Epoch 35/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3549 - mse: 0.3549\n",
      "Epoch 36/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.3538 - mse: 0.3538\n",
      "Epoch 37/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3215 - mse: 0.3215\n",
      "Epoch 38/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3365 - mse: 0.3365\n",
      "Epoch 39/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3284 - mse: 0.3284\n",
      "Epoch 40/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3087 - mse: 0.3087\n",
      "Epoch 41/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3318 - mse: 0.3318\n",
      "Epoch 42/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3614 - mse: 0.3614\n",
      "Epoch 43/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3115 - mse: 0.3115\n",
      "Epoch 44/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3159 - mse: 0.3159\n",
      "Epoch 45/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3282 - mse: 0.3282\n",
      "Epoch 46/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3166 - mse: 0.3166\n",
      "Epoch 47/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3200 - mse: 0.3200\n",
      "Epoch 48/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3028 - mse: 0.3028\n",
      "Epoch 49/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3348 - mse: 0.3348\n",
      "Epoch 50/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3295 - mse: 0.3295\n",
      "Epoch 51/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.2658 - mse: 0.2658\n",
      "Epoch 52/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3306 - mse: 0.3306\n",
      "Epoch 53/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2976 - mse: 0.2976\n",
      "Epoch 54/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3191 - mse: 0.3191\n",
      "Epoch 55/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.2955 - mse: 0.2955\n",
      "Epoch 56/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3028 - mse: 0.3028\n",
      "Epoch 57/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.2820 - mse: 0.2820\n",
      "Epoch 58/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2893 - mse: 0.2893\n",
      "Epoch 59/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3020 - mse: 0.3020\n",
      "Epoch 60/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2934 - mse: 0.2934\n",
      "Epoch 61/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2799 - mse: 0.2799\n",
      "Epoch 62/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2872 - mse: 0.2872\n",
      "Epoch 63/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2909 - mse: 0.2909\n",
      "Epoch 64/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2977 - mse: 0.2977\n",
      "Epoch 65/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.2701 - mse: 0.2701\n",
      "Epoch 66/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2734 - mse: 0.2734\n",
      "Epoch 67/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2755 - mse: 0.2755\n",
      "Epoch 68/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2819 - mse: 0.2819\n",
      "Epoch 69/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2774 - mse: 0.2774\n",
      "Epoch 70/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.2839 - mse: 0.2839\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e79e0b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e79e0b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:33:39.533614: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6515542587812815\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2abbeddc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2abbeddc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:34:53.318972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 7s 162ms/step - loss: 1.3090 - mse: 1.3090\n",
      "Epoch 2/70\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.6516 - mse: 0.6516\n",
      "Epoch 3/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.6055 - mse: 0.6055\n",
      "Epoch 4/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.5820 - mse: 0.5820\n",
      "Epoch 5/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.4709 - mse: 0.4709\n",
      "Epoch 6/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.5308 - mse: 0.5308\n",
      "Epoch 7/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.4833 - mse: 0.4833\n",
      "Epoch 8/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.4546 - mse: 0.4546\n",
      "Epoch 9/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4651 - mse: 0.4651\n",
      "Epoch 10/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4603 - mse: 0.4603\n",
      "Epoch 11/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.4840 - mse: 0.4840\n",
      "Epoch 12/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4731 - mse: 0.4731\n",
      "Epoch 13/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4354 - mse: 0.4354\n",
      "Epoch 14/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4610 - mse: 0.4610\n",
      "Epoch 15/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.4313 - mse: 0.4313\n",
      "Epoch 16/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4108 - mse: 0.4108\n",
      "Epoch 17/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3959 - mse: 0.3959\n",
      "Epoch 18/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4389 - mse: 0.4389\n",
      "Epoch 19/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.4004 - mse: 0.4004\n",
      "Epoch 20/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3826 - mse: 0.3826\n",
      "Epoch 21/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3974 - mse: 0.3974\n",
      "Epoch 22/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4290 - mse: 0.4290\n",
      "Epoch 23/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3704 - mse: 0.3704\n",
      "Epoch 24/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3903 - mse: 0.3903\n",
      "Epoch 25/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3627 - mse: 0.3627\n",
      "Epoch 26/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3803 - mse: 0.3803\n",
      "Epoch 27/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3600 - mse: 0.3600\n",
      "Epoch 28/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3804 - mse: 0.3804\n",
      "Epoch 29/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3638 - mse: 0.3638\n",
      "Epoch 30/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3717 - mse: 0.3717\n",
      "Epoch 31/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3609 - mse: 0.3609\n",
      "Epoch 32/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3678 - mse: 0.3678\n",
      "Epoch 33/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3592 - mse: 0.3592\n",
      "Epoch 34/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3630 - mse: 0.3630\n",
      "Epoch 35/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3583 - mse: 0.3583\n",
      "Epoch 36/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3312 - mse: 0.3312\n",
      "Epoch 37/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3796 - mse: 0.3796\n",
      "Epoch 38/70\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 0.3557 - mse: 0.3557\n",
      "Epoch 39/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3399 - mse: 0.3399\n",
      "Epoch 40/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3124 - mse: 0.3124\n",
      "Epoch 41/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3327 - mse: 0.3327\n",
      "Epoch 42/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3542 - mse: 0.3542\n",
      "Epoch 43/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3368 - mse: 0.3368\n",
      "Epoch 44/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3107 - mse: 0.3107\n",
      "Epoch 45/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3206 - mse: 0.3206\n",
      "Epoch 46/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3354 - mse: 0.3354\n",
      "Epoch 47/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.3028 - mse: 0.3028\n",
      "Epoch 48/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3169 - mse: 0.3169\n",
      "Epoch 49/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3208 - mse: 0.3208\n",
      "Epoch 50/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.2979 - mse: 0.2979\n",
      "Epoch 51/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3225 - mse: 0.3225\n",
      "Epoch 52/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3082 - mse: 0.3082\n",
      "Epoch 53/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3086 - mse: 0.3086\n",
      "Epoch 54/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3193 - mse: 0.3193\n",
      "Epoch 55/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2972 - mse: 0.2972\n",
      "Epoch 56/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.2808 - mse: 0.2808\n",
      "Epoch 57/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3160 - mse: 0.3160\n",
      "Epoch 58/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3233 - mse: 0.3233\n",
      "Epoch 59/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.2859 - mse: 0.2859\n",
      "Epoch 60/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3185 - mse: 0.3185\n",
      "Epoch 61/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2937 - mse: 0.2937\n",
      "Epoch 62/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3124 - mse: 0.3124\n",
      "Epoch 63/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2697 - mse: 0.2697\n",
      "Epoch 64/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2942 - mse: 0.2942\n",
      "Epoch 65/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.3057 - mse: 0.3057\n",
      "Epoch 66/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2598 - mse: 0.2598\n",
      "Epoch 67/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.2992 - mse: 0.2992\n",
      "Epoch 68/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.2753 - mse: 0.2753\n",
      "Epoch 69/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.2722 - mse: 0.2722\n",
      "Epoch 70/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.2697 - mse: 0.2697\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3054bce50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3054bce50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:35:46.509196: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5802919708029197\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab744e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab744e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:37:06.781725: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 155ms/step - loss: 1.3229 - mse: 1.3229\n",
      "Epoch 2/70\n",
      "11/11 [==============================] - 1s 88ms/step - loss: 0.5995 - mse: 0.5995\n",
      "Epoch 3/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.5593 - mse: 0.5593\n",
      "Epoch 4/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.5827 - mse: 0.5827\n",
      "Epoch 5/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.5080 - mse: 0.5080\n",
      "Epoch 6/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4558 - mse: 0.4558\n",
      "Epoch 7/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.5133 - mse: 0.5133\n",
      "Epoch 8/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.4674 - mse: 0.4674\n",
      "Epoch 9/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.4622 - mse: 0.4622\n",
      "Epoch 10/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.4640 - mse: 0.4640\n",
      "Epoch 11/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4182 - mse: 0.4182\n",
      "Epoch 12/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.4710 - mse: 0.4710\n",
      "Epoch 13/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.4204 - mse: 0.4204\n",
      "Epoch 14/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4458 - mse: 0.4458\n",
      "Epoch 15/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3863 - mse: 0.3863\n",
      "Epoch 16/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 17/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4084 - mse: 0.4084\n",
      "Epoch 18/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.3846 - mse: 0.3846\n",
      "Epoch 19/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.4163 - mse: 0.4163\n",
      "Epoch 20/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3882 - mse: 0.3882\n",
      "Epoch 21/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3434 - mse: 0.3434\n",
      "Epoch 22/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3998 - mse: 0.3998\n",
      "Epoch 23/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.3624 - mse: 0.3624\n",
      "Epoch 24/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3876 - mse: 0.3876\n",
      "Epoch 25/70\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.3669 - mse: 0.3669\n",
      "Epoch 26/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3585 - mse: 0.3585\n",
      "Epoch 27/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.4058 - mse: 0.4058\n",
      "Epoch 28/70\n",
      "11/11 [==============================] - 1s 62ms/step - loss: 0.3362 - mse: 0.3362\n",
      "Epoch 29/70\n",
      "11/11 [==============================] - 1s 59ms/step - loss: 0.3629 - mse: 0.3629\n",
      "Epoch 30/70\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.3434 - mse: 0.3434\n",
      "Epoch 31/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3408 - mse: 0.3408\n",
      "Epoch 32/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3531 - mse: 0.3531\n",
      "Epoch 33/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3581 - mse: 0.3581\n",
      "Epoch 34/70\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.3553 - mse: 0.3553\n",
      "Epoch 35/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3075 - mse: 0.3075\n",
      "Epoch 36/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.3612 - mse: 0.3612\n",
      "Epoch 37/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3238 - mse: 0.3238\n",
      "Epoch 38/70\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 0.3734 - mse: 0.3734\n",
      "Epoch 39/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.3125 - mse: 0.3125\n",
      "Epoch 40/70\n",
      "11/11 [==============================] - 1s 77ms/step - loss: 0.3495 - mse: 0.3495\n",
      "Epoch 41/70\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.3019 - mse: 0.3019\n",
      "Epoch 42/70\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.3466 - mse: 0.3466\n",
      "Epoch 43/70\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 0.3303 - mse: 0.3303\n",
      "Epoch 44/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.3336 - mse: 0.3336\n",
      "Epoch 45/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3234 - mse: 0.3234\n",
      "Epoch 46/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3171 - mse: 0.3171\n",
      "Epoch 47/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3241 - mse: 0.3241\n",
      "Epoch 48/70\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.2991 - mse: 0.2991\n",
      "Epoch 49/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3160 - mse: 0.3160\n",
      "Epoch 50/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3153 - mse: 0.3153\n",
      "Epoch 51/70\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 0.3224 - mse: 0.3224\n",
      "Epoch 52/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3218 - mse: 0.3218\n",
      "Epoch 53/70\n",
      "11/11 [==============================] - 1s 73ms/step - loss: 0.2774 - mse: 0.2774\n",
      "Epoch 54/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3008 - mse: 0.3008\n",
      "Epoch 55/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.3128 - mse: 0.3128\n",
      "Epoch 56/70\n",
      "11/11 [==============================] - 1s 65ms/step - loss: 0.3026 - mse: 0.3026\n",
      "Epoch 57/70\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 0.3117 - mse: 0.3117\n",
      "Epoch 58/70\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 0.3125 - mse: 0.3125\n",
      "Epoch 59/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.2612 - mse: 0.2612\n",
      "Epoch 60/70\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 0.2970 - mse: 0.2970\n",
      "Epoch 61/70\n",
      "11/11 [==============================] - 1s 76ms/step - loss: 0.2993 - mse: 0.2993\n",
      "Epoch 62/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.2749 - mse: 0.2749\n",
      "Epoch 63/70\n",
      "11/11 [==============================] - 1s 83ms/step - loss: 0.2844 - mse: 0.2844\n",
      "Epoch 64/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.2609 - mse: 0.2609\n",
      "Epoch 65/70\n",
      "11/11 [==============================] - 1s 72ms/step - loss: 0.2893 - mse: 0.2893\n",
      "Epoch 66/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.2644 - mse: 0.2644\n",
      "Epoch 67/70\n",
      "11/11 [==============================] - 1s 68ms/step - loss: 0.2823 - mse: 0.2823\n",
      "Epoch 68/70\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 0.2765 - mse: 0.2765\n",
      "Epoch 69/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.2812 - mse: 0.2812\n",
      "Epoch 70/70\n",
      "11/11 [==============================] - 1s 70ms/step - loss: 0.2752 - mse: 0.2752\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x34bbb6790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x34bbb6790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:38:04.213995: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5228137195172153\n",
      "Average kappa score value is : 0.5952267760150811\n",
      "\n",
      "--------SET 4--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2f62a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2f62a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:39:41.548845: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 192ms/step - loss: 1.7090 - mse: 1.7090\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6267 - mse: 0.6267\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6277 - mse: 0.6277\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5992 - mse: 0.5992\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5651 - mse: 0.5651\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4805 - mse: 0.4805\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4737 - mse: 0.4737\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4814 - mse: 0.4814\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4213 - mse: 0.4213\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4648 - mse: 0.4648\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3959 - mse: 0.3959\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4086 - mse: 0.4086\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4152 - mse: 0.4152\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4050 - mse: 0.4050\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4064 - mse: 0.4064\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3865 - mse: 0.3865\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3887 - mse: 0.3887\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3555 - mse: 0.3555\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3413 - mse: 0.3413\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3492 - mse: 0.3492\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3575 - mse: 0.3575\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3369 - mse: 0.3369\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3671 - mse: 0.3671\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3433 - mse: 0.3433\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2953 - mse: 0.2953\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3283 - mse: 0.3283\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3102 - mse: 0.3102\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3470 - mse: 0.3470\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3296 - mse: 0.3296\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3480 - mse: 0.3480\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2884 - mse: 0.2884\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3618 - mse: 0.3618\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3164 - mse: 0.3164\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2975 - mse: 0.2975\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2973 - mse: 0.2973\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2969 - mse: 0.2969\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3031 - mse: 0.3031\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2878 - mse: 0.2878\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2987 - mse: 0.2987\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3130 - mse: 0.3130\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2511 - mse: 0.2511\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2930 - mse: 0.2930\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3013 - mse: 0.3013\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2732 - mse: 0.2732\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3423 - mse: 0.3423\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2501 - mse: 0.2501\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2793 - mse: 0.2793\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2888 - mse: 0.2888\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3207 - mse: 0.3207\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2795 - mse: 0.2795\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2529 - mse: 0.2529\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2904 - mse: 0.2904\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2593 - mse: 0.2593\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2400 - mse: 0.2400\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2783 - mse: 0.2783\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2735 - mse: 0.2735\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2660 - mse: 0.2660\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2799 - mse: 0.2799\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2412 - mse: 0.2412\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2678 - mse: 0.2678\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2539 - mse: 0.2539\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2666 - mse: 0.2666\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2640 - mse: 0.2640\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2560 - mse: 0.2560\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2440 - mse: 0.2440\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2537 - mse: 0.2537\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2493 - mse: 0.2493\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2370 - mse: 0.2370\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2504 - mse: 0.2504\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6e5d040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6e5d040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:40:41.536924: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.70277771299298\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5920700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5920700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:41:57.480235: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 174ms/step - loss: 1.4217 - mse: 1.4217\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.7107 - mse: 0.7107\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5744 - mse: 0.5744\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5755 - mse: 0.5755\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4925 - mse: 0.4925\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5466 - mse: 0.5466\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4404 - mse: 0.4404\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4037 - mse: 0.4037\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4706 - mse: 0.4706\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4181 - mse: 0.4181\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4228 - mse: 0.4228\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4190 - mse: 0.4190\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4042 - mse: 0.4042\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3955 - mse: 0.3955\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3284 - mse: 0.3284\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3584 - mse: 0.3584\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3740 - mse: 0.3740\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3524 - mse: 0.3524\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.3357 - mse: 0.3357\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3302 - mse: 0.3302\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4099 - mse: 0.4099\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3316 - mse: 0.3316\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2978 - mse: 0.2978\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3000 - mse: 0.3000\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3741 - mse: 0.3741\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3219 - mse: 0.3219\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.2803 - mse: 0.2803\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3039 - mse: 0.3039\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3108 - mse: 0.3108\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2927 - mse: 0.2927\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2989 - mse: 0.2989\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3345 - mse: 0.3345\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3010 - mse: 0.3010\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2794 - mse: 0.2794\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.2958 - mse: 0.2958\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3015 - mse: 0.3015\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2985 - mse: 0.2985\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2978 - mse: 0.2978\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.2722 - mse: 0.2722\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2620 - mse: 0.2620\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.2997 - mse: 0.2997\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2758 - mse: 0.2758\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2874 - mse: 0.2874\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2822 - mse: 0.2822\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2986 - mse: 0.2986\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2627 - mse: 0.2627\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.2413 - mse: 0.2413\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.2839 - mse: 0.2839\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2662 - mse: 0.2662\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2763 - mse: 0.2763\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2699 - mse: 0.2699\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2414 - mse: 0.2414\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2344 - mse: 0.2344\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2657 - mse: 0.2657\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2607 - mse: 0.2607\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2286 - mse: 0.2286\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2724 - mse: 0.2724\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2439 - mse: 0.2439\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2551 - mse: 0.2551\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2176 - mse: 0.2176\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2550 - mse: 0.2550\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2406 - mse: 0.2406\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2551 - mse: 0.2551\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2057 - mse: 0.2057\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2389 - mse: 0.2389\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2395 - mse: 0.2395\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2257 - mse: 0.2257\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2253 - mse: 0.2253\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2322 - mse: 0.2322\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x176842af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x176842af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:43:00.916249: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7654461783106985\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d52aa8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d52aa8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:44:20.269392: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 182ms/step - loss: 1.6381 - mse: 1.6381\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.7333 - mse: 0.7333\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.6251 - mse: 0.6251\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5257 - mse: 0.5257\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.5487 - mse: 0.5487\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.5252 - mse: 0.5252\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.5432 - mse: 0.5432\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4509 - mse: 0.4509\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4069 - mse: 0.4069\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.4066 - mse: 0.4066\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.4644 - mse: 0.4644\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.4059 - mse: 0.4059\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.4259 - mse: 0.4259\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.3758 - mse: 0.3758\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4480 - mse: 0.4480\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.3794 - mse: 0.3794\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.3855 - mse: 0.3855\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.3436 - mse: 0.3436\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3678 - mse: 0.3678\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3759 - mse: 0.3759\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3764 - mse: 0.3764\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3377 - mse: 0.3377\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3615 - mse: 0.3615\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3043 - mse: 0.3043\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3315 - mse: 0.3315\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3283 - mse: 0.3283\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3361 - mse: 0.3361\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3415 - mse: 0.3415\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2857 - mse: 0.2857\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3573 - mse: 0.3573\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3129 - mse: 0.3129\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2962 - mse: 0.2962\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2789 - mse: 0.2789\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.2784 - mse: 0.2784\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3071 - mse: 0.3071\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2821 - mse: 0.2821\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3079 - mse: 0.3079\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3003 - mse: 0.3003\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2609 - mse: 0.2609\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3157 - mse: 0.3157\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2997 - mse: 0.2997\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2805 - mse: 0.2805\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2967 - mse: 0.2967\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2452 - mse: 0.2452\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2846 - mse: 0.2846\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2711 - mse: 0.2711\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2608 - mse: 0.2608\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.2556 - mse: 0.2556\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2860 - mse: 0.2860\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.2598 - mse: 0.2598\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2565 - mse: 0.2565\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2749 - mse: 0.2749\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2341 - mse: 0.2341\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2638 - mse: 0.2638\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2609 - mse: 0.2609\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.2629 - mse: 0.2629\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2324 - mse: 0.2324\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2704 - mse: 0.2704\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2578 - mse: 0.2578\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2221 - mse: 0.2221\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.2583 - mse: 0.2583\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2306 - mse: 0.2306\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.2437 - mse: 0.2437\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2310 - mse: 0.2310\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2454 - mse: 0.2454\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2167 - mse: 0.2167\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2335 - mse: 0.2335\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.2389 - mse: 0.2389\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2304 - mse: 0.2304\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2429 - mse: 0.2429\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2955ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2955ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:45:24.725541: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7524439269787571\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x38ef2fdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x38ef2fdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:46:48.415915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 210ms/step - loss: 1.4126 - mse: 1.4126\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.7049 - mse: 0.7049\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6908 - mse: 0.6908\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5267 - mse: 0.5267\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5216 - mse: 0.5216\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5510 - mse: 0.5510\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5189 - mse: 0.5189\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4659 - mse: 0.4659\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4435 - mse: 0.4435\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4604 - mse: 0.4604\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3686 - mse: 0.3686\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4383 - mse: 0.4383\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4170 - mse: 0.4170\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3702 - mse: 0.3702\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3719 - mse: 0.3719\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4382 - mse: 0.4382\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3469 - mse: 0.3469\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3680 - mse: 0.3680\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3411 - mse: 0.3411\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3865 - mse: 0.3865\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3639 - mse: 0.3639\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3659 - mse: 0.3659\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3710 - mse: 0.3710\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3384 - mse: 0.3384\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3328 - mse: 0.3328\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3416 - mse: 0.3416\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3338 - mse: 0.3338\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3147 - mse: 0.3147\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3094 - mse: 0.3094\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3246 - mse: 0.3246\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3382 - mse: 0.3382\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3001 - mse: 0.3001\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3682 - mse: 0.3682\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2551 - mse: 0.2551\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3430 - mse: 0.3430\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2884 - mse: 0.2884\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2879 - mse: 0.2879\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3232 - mse: 0.3232\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3174 - mse: 0.3174\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.2887 - mse: 0.2887\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2983 - mse: 0.2983\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2611 - mse: 0.2611\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2720 - mse: 0.2720\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3116 - mse: 0.3116\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2825 - mse: 0.2825\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.2797 - mse: 0.2797\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2922 - mse: 0.2922\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2820 - mse: 0.2820\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.2821 - mse: 0.2821\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2448 - mse: 0.2448\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2982 - mse: 0.2982\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2923 - mse: 0.2923\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2502 - mse: 0.2502\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2777 - mse: 0.2777\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2752 - mse: 0.2752\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2439 - mse: 0.2439\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2695 - mse: 0.2695\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2243 - mse: 0.2243\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2748 - mse: 0.2748\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2619 - mse: 0.2619\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2338 - mse: 0.2338\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2542 - mse: 0.2542\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2264 - mse: 0.2264\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2630 - mse: 0.2630\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2636 - mse: 0.2636\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2451 - mse: 0.2451\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2540 - mse: 0.2540\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2312 - mse: 0.2312\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e5672040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e5672040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:47:50.637963: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7701703093716229\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2ad5280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2ad5280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:49:10.505940: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 197ms/step - loss: 1.2966 - mse: 1.2966\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6480 - mse: 0.6480\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.5771 - mse: 0.5771\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5803 - mse: 0.5803\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5192 - mse: 0.5192\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4354 - mse: 0.4354\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4797 - mse: 0.4797\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4819 - mse: 0.4819\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4125 - mse: 0.4125\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3588 - mse: 0.3588\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4470 - mse: 0.4470\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4044 - mse: 0.4044\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4047 - mse: 0.4047\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3638 - mse: 0.3638\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3469 - mse: 0.3469\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3516 - mse: 0.3516\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3702 - mse: 0.3702\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3758 - mse: 0.3758\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3127 - mse: 0.3127\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3491 - mse: 0.3491\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3333 - mse: 0.3333\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3680 - mse: 0.3680\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3653 - mse: 0.3653\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3220 - mse: 0.3220\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3049 - mse: 0.3049\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3125 - mse: 0.3125\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3552 - mse: 0.3552\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3539 - mse: 0.3539\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3061 - mse: 0.3061\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3500 - mse: 0.3500\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3035 - mse: 0.3035\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3092 - mse: 0.3092\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3143 - mse: 0.3143\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2939 - mse: 0.2939\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3006 - mse: 0.3006\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3105 - mse: 0.3105\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2992 - mse: 0.2992\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3129 - mse: 0.3129\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2812 - mse: 0.2812\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2597 - mse: 0.2597\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2937 - mse: 0.2937\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2463 - mse: 0.2463\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3149 - mse: 0.3149\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2761 - mse: 0.2761\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2852 - mse: 0.2852\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2531 - mse: 0.2531\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2553 - mse: 0.2553\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2439 - mse: 0.2439\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2547 - mse: 0.2547\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2748 - mse: 0.2748\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2404 - mse: 0.2404\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2526 - mse: 0.2526\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2635 - mse: 0.2635\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2374 - mse: 0.2374\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2778 - mse: 0.2778\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2284 - mse: 0.2284\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2573 - mse: 0.2573\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2405 - mse: 0.2405\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2377 - mse: 0.2377\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2378 - mse: 0.2378\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2450 - mse: 0.2450\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2357 - mse: 0.2357\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2351 - mse: 0.2351\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2545 - mse: 0.2545\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2550 - mse: 0.2550\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2224 - mse: 0.2224\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2532 - mse: 0.2532\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2148 - mse: 0.2148\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2356 - mse: 0.2356\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2489 - mse: 0.2489\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6f76550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6f76550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:50:10.495503: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7031772292939866\n",
      "Average kappa score value is : 0.738803071389609\n",
      "\n",
      "--------SET 5--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d52aa310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d52aa310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:51:41.540113: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 220ms/step - loss: 1.7920 - mse: 1.7920\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.8652 - mse: 0.8652\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.7321 - mse: 0.7321\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6369 - mse: 0.6369\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.6867 - mse: 0.6867\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.5947 - mse: 0.5947\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5847 - mse: 0.5847\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5172 - mse: 0.5172\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.6209 - mse: 0.6209\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5544 - mse: 0.5544\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5236 - mse: 0.5236\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.4707 - mse: 0.4707\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5246 - mse: 0.5246\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4920 - mse: 0.4920\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5235 - mse: 0.5235\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4838 - mse: 0.4838\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4616 - mse: 0.4616\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4619 - mse: 0.4619\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4508 - mse: 0.4508\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4725 - mse: 0.4725\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4447 - mse: 0.4447\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4287 - mse: 0.4287\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4270 - mse: 0.4270\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4070 - mse: 0.4070\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4314 - mse: 0.4314\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4419 - mse: 0.4419\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4220 - mse: 0.4220\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4001 - mse: 0.4001\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4089 - mse: 0.4089\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4090 - mse: 0.4090\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4151 - mse: 0.4151\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3825 - mse: 0.3825\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4011 - mse: 0.4011\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.4088 - mse: 0.4088\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3672 - mse: 0.3672\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3969 - mse: 0.3969\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3962 - mse: 0.3962\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3560 - mse: 0.3560\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3637 - mse: 0.3637\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4026 - mse: 0.4026\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3417 - mse: 0.3417\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3558 - mse: 0.3558\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3754 - mse: 0.3754\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3116 - mse: 0.3116\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3527 - mse: 0.3527\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3793 - mse: 0.3793\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3598 - mse: 0.3598\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3398 - mse: 0.3398\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3079 - mse: 0.3079\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3471 - mse: 0.3471\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3768 - mse: 0.3768\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3205 - mse: 0.3205\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3284 - mse: 0.3284\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3035 - mse: 0.3035\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3535 - mse: 0.3535\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3296 - mse: 0.3296\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3315 - mse: 0.3315\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2865 - mse: 0.2865\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3103 - mse: 0.3103\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3406 - mse: 0.3406\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3203 - mse: 0.3203\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3130 - mse: 0.3130\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3064 - mse: 0.3064\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3257 - mse: 0.3257\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3108 - mse: 0.3108\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3194 - mse: 0.3194\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3156 - mse: 0.3156\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3223 - mse: 0.3223\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2757 - mse: 0.2757\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a8bb7d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a8bb7d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:52:43.648587: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6969602486676805\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x169cf31f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x169cf31f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:54:10.965623: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 226ms/step - loss: 1.9996 - mse: 1.9996\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.8892 - mse: 0.8892\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.6234 - mse: 0.6234\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.7085 - mse: 0.7085\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6970 - mse: 0.6970\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.7403 - mse: 0.7403\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.6095 - mse: 0.6095\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5886 - mse: 0.5886\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.6241 - mse: 0.6241\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5796 - mse: 0.5796\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.6116 - mse: 0.6116\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5370 - mse: 0.5370\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5453 - mse: 0.5453\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5055 - mse: 0.5055\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4932 - mse: 0.4932\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5139 - mse: 0.5139\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5215 - mse: 0.5215\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4395 - mse: 0.4395\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4806 - mse: 0.4806\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4908 - mse: 0.4908\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4555 - mse: 0.4555\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4702 - mse: 0.4702\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4305 - mse: 0.4305\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4254 - mse: 0.4254\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3777 - mse: 0.3777\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5031 - mse: 0.5031\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4012 - mse: 0.4012\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4030 - mse: 0.4030\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4209 - mse: 0.4209\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4182 - mse: 0.4182\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4190 - mse: 0.4190\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3487 - mse: 0.3487\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4141 - mse: 0.4141\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3723 - mse: 0.3723\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3946 - mse: 0.3946\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4040 - mse: 0.4040\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3976 - mse: 0.3976\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3614 - mse: 0.3614\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3948 - mse: 0.3948\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3773 - mse: 0.3773\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3722 - mse: 0.3722\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3634 - mse: 0.3634\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3830 - mse: 0.3830\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3498 - mse: 0.3498\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3796 - mse: 0.3796\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3071 - mse: 0.3071\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3675 - mse: 0.3675\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3491 - mse: 0.3491\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3712 - mse: 0.3712\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3349 - mse: 0.3349\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3425 - mse: 0.3425\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3169 - mse: 0.3169\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3613 - mse: 0.3613\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3277 - mse: 0.3277\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3125 - mse: 0.3125\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3266 - mse: 0.3266\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3295 - mse: 0.3295\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3227 - mse: 0.3227\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3209 - mse: 0.3209\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3249 - mse: 0.3249\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3014 - mse: 0.3014\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3435 - mse: 0.3435\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3496 - mse: 0.3496\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3003 - mse: 0.3003\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3009 - mse: 0.3009\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3131 - mse: 0.3131\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3101 - mse: 0.3101\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3180 - mse: 0.3180\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3396c65e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3396c65e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:55:15.923210: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7419051687313114\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b28b28b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b28b28b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:56:39.079971: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 211ms/step - loss: 2.1088 - mse: 2.1088\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.8988 - mse: 0.8988\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6677 - mse: 0.6677\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6847 - mse: 0.6847\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.6829 - mse: 0.6829\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5817 - mse: 0.5817\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5625 - mse: 0.5625\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.6495 - mse: 0.6495\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5725 - mse: 0.5725\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.5658 - mse: 0.5658\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.4691 - mse: 0.4691\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5014 - mse: 0.5014\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.4735 - mse: 0.4735\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4935 - mse: 0.4935\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.5501 - mse: 0.5501\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.4668 - mse: 0.4668\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4508 - mse: 0.4508\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4283 - mse: 0.4283\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.4911 - mse: 0.4911\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.4296 - mse: 0.4296\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3868 - mse: 0.3868\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.4609 - mse: 0.4609\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4352 - mse: 0.4352\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4634 - mse: 0.4634\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3709 - mse: 0.3709\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4428 - mse: 0.4428\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.3478 - mse: 0.3478\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.4221 - mse: 0.4221\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.4306 - mse: 0.4306\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.3437 - mse: 0.3437\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.4446 - mse: 0.4446\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.4262 - mse: 0.4262\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3579 - mse: 0.3579\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3938 - mse: 0.3938\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3951 - mse: 0.3951\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4078 - mse: 0.4078\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3654 - mse: 0.3654\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3607 - mse: 0.3607\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3944 - mse: 0.3944\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3539 - mse: 0.3539\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3785 - mse: 0.3785\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3488 - mse: 0.3488\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3491 - mse: 0.3491\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3614 - mse: 0.3614\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2910 - mse: 0.2910\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3352 - mse: 0.3352\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3252 - mse: 0.3252\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3329 - mse: 0.3329\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3100 - mse: 0.3100\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3379 - mse: 0.3379\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3419 - mse: 0.3419\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3350 - mse: 0.3350\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3130 - mse: 0.3130\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3337 - mse: 0.3337\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3039 - mse: 0.3039\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3107 - mse: 0.3107\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3083 - mse: 0.3083\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3300 - mse: 0.3300\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3296 - mse: 0.3296\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3310 - mse: 0.3310\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2787 - mse: 0.2787\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3026 - mse: 0.3026\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3008 - mse: 0.3008\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3052 - mse: 0.3052\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2691 - mse: 0.2691\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3360 - mse: 0.3360\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3339 - mse: 0.3339\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3118 - mse: 0.3118\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6e5d280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b6e5d280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:57:44.900255: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7295730100158144\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fd5ae670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fd5ae670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 23:59:03.426337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 210ms/step - loss: 2.1853 - mse: 2.1853\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 0.8528 - mse: 0.8528\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.7729 - mse: 0.7729\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6580 - mse: 0.6580\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.6546 - mse: 0.6546\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5745 - mse: 0.5745\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5679 - mse: 0.5679\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6363 - mse: 0.6363\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5704 - mse: 0.5704\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5430 - mse: 0.5430\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5426 - mse: 0.5426\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4825 - mse: 0.4825\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5265 - mse: 0.5265\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5072 - mse: 0.5072\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5235 - mse: 0.5235\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4806 - mse: 0.4806\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4476 - mse: 0.4476\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4612 - mse: 0.4612\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4593 - mse: 0.4593\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4616 - mse: 0.4616\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4580 - mse: 0.4580\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3971 - mse: 0.3971\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4464 - mse: 0.4464\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4122 - mse: 0.4122\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4461 - mse: 0.4461\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4239 - mse: 0.4239\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4430 - mse: 0.4430\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3777 - mse: 0.3777\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3891 - mse: 0.3891\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4278 - mse: 0.4278\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3816 - mse: 0.3816\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4111 - mse: 0.4111\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3498 - mse: 0.3498\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4306 - mse: 0.4306\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4160 - mse: 0.4160\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3868 - mse: 0.3868\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3894 - mse: 0.3894\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3860 - mse: 0.3860\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3336 - mse: 0.3336\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3975 - mse: 0.3975\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3231 - mse: 0.3231\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3873 - mse: 0.3873\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3696 - mse: 0.3696\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3888 - mse: 0.3888\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3609 - mse: 0.3609\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3519 - mse: 0.3519\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3433 - mse: 0.3433\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3649 - mse: 0.3649\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3501 - mse: 0.3501\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3234 - mse: 0.3234\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3783 - mse: 0.3783\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3189 - mse: 0.3189\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3416 - mse: 0.3416\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3430 - mse: 0.3430\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3395 - mse: 0.3395\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3573 - mse: 0.3573\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3581 - mse: 0.3581\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3088 - mse: 0.3088\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3424 - mse: 0.3424\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3325 - mse: 0.3325\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2981 - mse: 0.2981\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3262 - mse: 0.3262\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3448 - mse: 0.3448\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3281 - mse: 0.3281\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3000 - mse: 0.3000\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2873 - mse: 0.2873\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3171 - mse: 0.3171\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3084 - mse: 0.3084\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3266 - mse: 0.3266\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2c48585e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2c48585e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:00:06.623920: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7471326442767847\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b711caf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b711caf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:01:24.578633: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 222ms/step - loss: 2.1989 - mse: 2.1989\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.8237 - mse: 0.8237\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6815 - mse: 0.6815\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.8039 - mse: 0.8039\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.6956 - mse: 0.6956\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.6425 - mse: 0.6425\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6110 - mse: 0.6110\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5792 - mse: 0.5792\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5469 - mse: 0.5469\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5534 - mse: 0.5534\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5214 - mse: 0.5214\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5126 - mse: 0.5126\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4999 - mse: 0.4999\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5053 - mse: 0.5053\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4884 - mse: 0.4884\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3838 - mse: 0.3838\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5340 - mse: 0.5340\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4582 - mse: 0.4582\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4508 - mse: 0.4508\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4429 - mse: 0.4429\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4110 - mse: 0.4110\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4667 - mse: 0.4667\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3864 - mse: 0.3864\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3819 - mse: 0.3819\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4542 - mse: 0.4542\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4554 - mse: 0.4554\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3920 - mse: 0.3920\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4616 - mse: 0.4616\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4030 - mse: 0.4030\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4289 - mse: 0.4289\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3697 - mse: 0.3697\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3794 - mse: 0.3794\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.3973 - mse: 0.3973\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 0.4393 - mse: 0.4393\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3687 - mse: 0.3687\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3063 - mse: 0.3063\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4115 - mse: 0.4115\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3902 - mse: 0.3902\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3367 - mse: 0.3367\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3624 - mse: 0.3624\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3675 - mse: 0.3675\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3327 - mse: 0.3327\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4162 - mse: 0.4162\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3203 - mse: 0.3203\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3272 - mse: 0.3272\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3328 - mse: 0.3328\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3636 - mse: 0.3636\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3517 - mse: 0.3517\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3218 - mse: 0.3218\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3283 - mse: 0.3283\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3270 - mse: 0.3270\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3343 - mse: 0.3343\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3457 - mse: 0.3457\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3120 - mse: 0.3120\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3308 - mse: 0.3308\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3499 - mse: 0.3499\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3338 - mse: 0.3338\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3323 - mse: 0.3323\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3070 - mse: 0.3070\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3077 - mse: 0.3077\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2920 - mse: 0.2920\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3120 - mse: 0.3120\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2807 - mse: 0.2807\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3146 - mse: 0.3146\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3022 - mse: 0.3022\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2982 - mse: 0.2982\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3010 - mse: 0.3010\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.2613 - mse: 0.2613\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3156 - mse: 0.3156\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3117 - mse: 0.3117\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x176a5ca60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x176a5ca60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:02:31.314231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.72801785157281\n",
      "Average kappa score value is : 0.7287177846528803\n",
      "\n",
      "--------SET 6--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5e97dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2e5e97dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:03:59.793397: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 223ms/step - loss: 2.4650 - mse: 2.4650\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 0.9490 - mse: 0.9490\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.8210 - mse: 0.8210\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.9093 - mse: 0.9093\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.8854 - mse: 0.8854\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6924 - mse: 0.6924\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.6337 - mse: 0.6337\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.7136 - mse: 0.7136\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5551 - mse: 0.5551\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6701 - mse: 0.6701\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5756 - mse: 0.5756\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.6213 - mse: 0.6213\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5873 - mse: 0.5873\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4600 - mse: 0.4600\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5433 - mse: 0.5433\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4819 - mse: 0.4819\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5047 - mse: 0.5047\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5233 - mse: 0.5233\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5083 - mse: 0.5083\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5037 - mse: 0.5037\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.5039 - mse: 0.5039\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.5191 - mse: 0.5191\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4679 - mse: 0.4679\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4613 - mse: 0.4613\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4873 - mse: 0.4873\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4911 - mse: 0.4911\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4296 - mse: 0.4296\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5384 - mse: 0.5384\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4106 - mse: 0.4106\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4380 - mse: 0.4380\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4646 - mse: 0.4646\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4330 - mse: 0.4330\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4135 - mse: 0.4135\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4781 - mse: 0.4781\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4083 - mse: 0.4083\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4321 - mse: 0.4321\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4031 - mse: 0.4031\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4056 - mse: 0.4056\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4810 - mse: 0.4810\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3924 - mse: 0.3924\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4202 - mse: 0.4202\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3696 - mse: 0.3696\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3806 - mse: 0.3806\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3972 - mse: 0.3972\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4213 - mse: 0.4213\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3946 - mse: 0.3946\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3666 - mse: 0.3666\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3814 - mse: 0.3814\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3765 - mse: 0.3765\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3708 - mse: 0.3708\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3876 - mse: 0.3876\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3729 - mse: 0.3729\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3565 - mse: 0.3565\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3751 - mse: 0.3751\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3886 - mse: 0.3886\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3804 - mse: 0.3804\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3409 - mse: 0.3409\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3703 - mse: 0.3703\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2851 - mse: 0.2851\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4061 - mse: 0.4061\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3220 - mse: 0.3220\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3589 - mse: 0.3589\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.3239 - mse: 0.3239\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3540 - mse: 0.3540\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3146 - mse: 0.3146\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.3206 - mse: 0.3206\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3511 - mse: 0.3511\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2791 - mse: 0.2791\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3581 - mse: 0.3581\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3314 - mse: 0.3314\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3dc1e6ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3dc1e6ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:05:03.864415: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6941668509842559\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x169d28160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x169d28160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:06:24.753904: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 221ms/step - loss: 2.4278 - mse: 2.4278\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.9028 - mse: 0.9028\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.9222 - mse: 0.9222\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.8361 - mse: 0.8361\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.7464 - mse: 0.7464\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6832 - mse: 0.6832\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.6467 - mse: 0.6467\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5800 - mse: 0.5800\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6601 - mse: 0.6601\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.6284 - mse: 0.6284\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5520 - mse: 0.5520\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5841 - mse: 0.5841\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.6263 - mse: 0.6263\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5380 - mse: 0.5380\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5084 - mse: 0.5084\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5409 - mse: 0.5409\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5464 - mse: 0.5464\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4655 - mse: 0.4655\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5333 - mse: 0.5333\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5530 - mse: 0.5530\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4886 - mse: 0.4886\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4763 - mse: 0.4763\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4596 - mse: 0.4596\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5300 - mse: 0.5300\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4632 - mse: 0.4632\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4938 - mse: 0.4938\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4924 - mse: 0.4924\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4324 - mse: 0.4324\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4527 - mse: 0.4527\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4880 - mse: 0.4880\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4752 - mse: 0.4752\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4403 - mse: 0.4403\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4173 - mse: 0.4173\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4509 - mse: 0.4509\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3950 - mse: 0.3950\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3852 - mse: 0.3852\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4493 - mse: 0.4493\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4117 - mse: 0.4117\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4279 - mse: 0.4279\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4477 - mse: 0.4477\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4016 - mse: 0.4016\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4090 - mse: 0.4090\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.4131 - mse: 0.4131\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3981 - mse: 0.3981\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.3683 - mse: 0.3683\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.4445 - mse: 0.4445\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4089 - mse: 0.4089\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3999 - mse: 0.3999\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3852 - mse: 0.3852\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3814 - mse: 0.3814\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3598 - mse: 0.3598\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3791 - mse: 0.3791\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4491 - mse: 0.4491\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3575 - mse: 0.3575\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3624 - mse: 0.3624\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3759 - mse: 0.3759\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3915 - mse: 0.3915\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3427 - mse: 0.3427\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3907 - mse: 0.3907\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3516 - mse: 0.3516\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.3788 - mse: 0.3788\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.3615 - mse: 0.3615\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.3414 - mse: 0.3414\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3787 - mse: 0.3787\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3457 - mse: 0.3457\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.3837 - mse: 0.3837\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3180 - mse: 0.3180\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3635 - mse: 0.3635\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.3500 - mse: 0.3500\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3328 - mse: 0.3328\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d52aa700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2d52aa700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:07:30.775759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6938606652325003\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2627b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b2627b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:08:56.591002: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 227ms/step - loss: 2.5105 - mse: 2.5105\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.8826 - mse: 0.8826\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.9085 - mse: 0.9085\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7927 - mse: 0.7927\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.7779 - mse: 0.7779\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.6752 - mse: 0.6752\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.6288 - mse: 0.6288\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6401 - mse: 0.6401\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.6026 - mse: 0.6026\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6142 - mse: 0.6142\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5558 - mse: 0.5558\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5782 - mse: 0.5782\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.6110 - mse: 0.6110\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4893 - mse: 0.4893\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5231 - mse: 0.5231\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5199 - mse: 0.5199\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4703 - mse: 0.4703\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5229 - mse: 0.5229\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5395 - mse: 0.5395\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4321 - mse: 0.4321\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5590 - mse: 0.5590\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4841 - mse: 0.4841\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4437 - mse: 0.4437\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4776 - mse: 0.4776\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4883 - mse: 0.4883\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4440 - mse: 0.4440\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4644 - mse: 0.4644\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4912 - mse: 0.4912\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4140 - mse: 0.4140\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4210 - mse: 0.4210\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4490 - mse: 0.4490\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4012 - mse: 0.4012\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4631 - mse: 0.4631\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4015 - mse: 0.4015\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3663 - mse: 0.3663\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4252 - mse: 0.4252\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3962 - mse: 0.3962\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3889 - mse: 0.3889\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4118 - mse: 0.4118\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4371 - mse: 0.4371\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4163 - mse: 0.4163\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4254 - mse: 0.4254\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4358 - mse: 0.4358\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3817 - mse: 0.3817\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4021 - mse: 0.4021\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3943 - mse: 0.3943\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4325 - mse: 0.4325\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3804 - mse: 0.3804\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3790 - mse: 0.3790\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3377 - mse: 0.3377\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4378 - mse: 0.4378\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3958 - mse: 0.3958\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3658 - mse: 0.3658\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3964 - mse: 0.3964\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3689 - mse: 0.3689\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3851 - mse: 0.3851\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3565 - mse: 0.3565\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3612 - mse: 0.3612\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3533 - mse: 0.3533\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3334 - mse: 0.3334\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3555 - mse: 0.3555\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3756 - mse: 0.3756\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3570 - mse: 0.3570\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.3524 - mse: 0.3524\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3317 - mse: 0.3317\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3571 - mse: 0.3571\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3396 - mse: 0.3396\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3959 - mse: 0.3959\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3144 - mse: 0.3144\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3618 - mse: 0.3618\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2858ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2858ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:10:02.153729: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6774337169697915\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3054cc5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3054cc5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:11:20.955653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 236ms/step - loss: 2.3247 - mse: 2.3247\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 0.9667 - mse: 0.9667\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.8623 - mse: 0.8623\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.8147 - mse: 0.8147\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6860 - mse: 0.6860\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7325 - mse: 0.7325\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6510 - mse: 0.6510\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7285 - mse: 0.7285\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.5671 - mse: 0.5671\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5546 - mse: 0.5546\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5816 - mse: 0.5816\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4970 - mse: 0.4970\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5261 - mse: 0.5261\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5434 - mse: 0.5434\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.5673 - mse: 0.5673\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4927 - mse: 0.4927\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5355 - mse: 0.5355\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4789 - mse: 0.4789\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4849 - mse: 0.4849\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4671 - mse: 0.4671\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4550 - mse: 0.4550\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5283 - mse: 0.5283\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4195 - mse: 0.4195\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4714 - mse: 0.4714\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4984 - mse: 0.4984\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4814 - mse: 0.4814\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4575 - mse: 0.4575\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4212 - mse: 0.4212\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4262 - mse: 0.4262\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4537 - mse: 0.4537\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4259 - mse: 0.4259\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3653 - mse: 0.3653\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.4228 - mse: 0.4228\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4020 - mse: 0.4020\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4226 - mse: 0.4226\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3869 - mse: 0.3869\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4619 - mse: 0.4619\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3601 - mse: 0.3601\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3822 - mse: 0.3822\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3979 - mse: 0.3979\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3598 - mse: 0.3598\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3870 - mse: 0.3870\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4012 - mse: 0.4012\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3718 - mse: 0.3718\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3810 - mse: 0.3810\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3743 - mse: 0.3743\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3646 - mse: 0.3646\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3694 - mse: 0.3694\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3611 - mse: 0.3611\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3306 - mse: 0.3306\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3945 - mse: 0.3945\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3427 - mse: 0.3427\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3860 - mse: 0.3860\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3454 - mse: 0.3454\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3492 - mse: 0.3492\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3130 - mse: 0.3130\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3553 - mse: 0.3553\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3330 - mse: 0.3330\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3517 - mse: 0.3517\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3543 - mse: 0.3543\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3096 - mse: 0.3096\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3316 - mse: 0.3316\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2784 - mse: 0.2784\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3686 - mse: 0.3686\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3069 - mse: 0.3069\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3442 - mse: 0.3442\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3081 - mse: 0.3081\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a8dea550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2a8dea550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:12:26.757087: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5907602764641688\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338acef70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338acef70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:13:43.658920: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 231ms/step - loss: 2.4262 - mse: 2.4262\n",
      "Epoch 2/70\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.9407 - mse: 0.9407\n",
      "Epoch 3/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.8682 - mse: 0.8682\n",
      "Epoch 4/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.9214 - mse: 0.9214\n",
      "Epoch 5/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7290 - mse: 0.7290\n",
      "Epoch 6/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.7107 - mse: 0.7107\n",
      "Epoch 7/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6654 - mse: 0.6654\n",
      "Epoch 8/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5969 - mse: 0.5969\n",
      "Epoch 9/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5991 - mse: 0.5991\n",
      "Epoch 10/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.6221 - mse: 0.6221\n",
      "Epoch 11/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.5763 - mse: 0.5763\n",
      "Epoch 12/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.6042 - mse: 0.6042\n",
      "Epoch 13/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.6142 - mse: 0.6142\n",
      "Epoch 14/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.5499 - mse: 0.5499\n",
      "Epoch 15/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4243 - mse: 0.4243\n",
      "Epoch 16/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.5882 - mse: 0.5882\n",
      "Epoch 17/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4426 - mse: 0.4426\n",
      "Epoch 18/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.5183 - mse: 0.5183\n",
      "Epoch 19/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.5501 - mse: 0.5501\n",
      "Epoch 20/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4787 - mse: 0.4787\n",
      "Epoch 21/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.5012 - mse: 0.5012\n",
      "Epoch 22/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.5366 - mse: 0.5366\n",
      "Epoch 23/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4502 - mse: 0.4502\n",
      "Epoch 24/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5075 - mse: 0.5075\n",
      "Epoch 25/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.3780 - mse: 0.3780\n",
      "Epoch 26/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.5175 - mse: 0.5175\n",
      "Epoch 27/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4354 - mse: 0.4354\n",
      "Epoch 28/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4183 - mse: 0.4183\n",
      "Epoch 29/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4269 - mse: 0.4269\n",
      "Epoch 30/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4684 - mse: 0.4684\n",
      "Epoch 31/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4084 - mse: 0.4084\n",
      "Epoch 32/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4735 - mse: 0.4735\n",
      "Epoch 33/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.4036 - mse: 0.4036\n",
      "Epoch 34/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4620 - mse: 0.4620\n",
      "Epoch 35/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4154 - mse: 0.4154\n",
      "Epoch 36/70\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.4619 - mse: 0.4619\n",
      "Epoch 37/70\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4358 - mse: 0.4358\n",
      "Epoch 38/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4269 - mse: 0.4269\n",
      "Epoch 39/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3912 - mse: 0.3912\n",
      "Epoch 40/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.4508 - mse: 0.4508\n",
      "Epoch 41/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3871 - mse: 0.3871\n",
      "Epoch 42/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4073 - mse: 0.4073\n",
      "Epoch 43/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3794 - mse: 0.3794\n",
      "Epoch 44/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4428 - mse: 0.4428\n",
      "Epoch 45/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3903 - mse: 0.3903\n",
      "Epoch 46/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3983 - mse: 0.3983\n",
      "Epoch 47/70\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.3977 - mse: 0.3977\n",
      "Epoch 48/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.4079 - mse: 0.4079\n",
      "Epoch 49/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.3663 - mse: 0.3663\n",
      "Epoch 50/70\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.4057 - mse: 0.4057\n",
      "Epoch 51/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4003 - mse: 0.4003\n",
      "Epoch 52/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3227 - mse: 0.3227\n",
      "Epoch 53/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4307 - mse: 0.4307\n",
      "Epoch 54/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3989 - mse: 0.3989\n",
      "Epoch 55/70\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.3624 - mse: 0.3624\n",
      "Epoch 56/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3457 - mse: 0.3457\n",
      "Epoch 57/70\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 58/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.4105 - mse: 0.4105\n",
      "Epoch 59/70\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.3586 - mse: 0.3586\n",
      "Epoch 60/70\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3520 - mse: 0.3520\n",
      "Epoch 61/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3553 - mse: 0.3553\n",
      "Epoch 62/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3601 - mse: 0.3601\n",
      "Epoch 63/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3408 - mse: 0.3408\n",
      "Epoch 64/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3699 - mse: 0.3699\n",
      "Epoch 65/70\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.3051 - mse: 0.3051\n",
      "Epoch 66/70\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.3507 - mse: 0.3507\n",
      "Epoch 67/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3478 - mse: 0.3478\n",
      "Epoch 68/70\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.3126 - mse: 0.3126\n",
      "Epoch 69/70\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.3402 - mse: 0.3402\n",
      "Epoch 70/70\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.3287 - mse: 0.3287\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab1cd940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab1cd940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:14:48.937516: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6994379734462817\n",
      "Average kappa score value is : 0.6711318966193996\n",
      "\n",
      "--------SET 7--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338dcea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338dcea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:16:15.835357: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 9s 259ms/step - loss: 87.4496 - mse: 87.4496\n",
      "Epoch 2/70\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 21.5509 - mse: 21.5509\n",
      "Epoch 3/70\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 19.9467 - mse: 19.9467\n",
      "Epoch 4/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 14.6436 - mse: 14.6436\n",
      "Epoch 5/70\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 15.5975 - mse: 15.5975\n",
      "Epoch 6/70\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 13.4208 - mse: 13.4208\n",
      "Epoch 7/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 12.7519 - mse: 12.7519\n",
      "Epoch 8/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 13.9668 - mse: 13.9668\n",
      "Epoch 9/70\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 14.2822 - mse: 14.2822\n",
      "Epoch 10/70\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 11.2260 - mse: 11.2260\n",
      "Epoch 11/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 12.3833 - mse: 12.3833\n",
      "Epoch 12/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 12.3536 - mse: 12.3536\n",
      "Epoch 13/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 11.0272 - mse: 11.0272\n",
      "Epoch 14/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 11.6824 - mse: 11.6824\n",
      "Epoch 15/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.4161 - mse: 9.4161\n",
      "Epoch 16/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 11.0801 - mse: 11.0801\n",
      "Epoch 17/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.3440 - mse: 10.3440\n",
      "Epoch 18/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.9494 - mse: 8.9494\n",
      "Epoch 19/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.7924 - mse: 10.7924\n",
      "Epoch 20/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.0235 - mse: 10.0235\n",
      "Epoch 21/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 10.0811 - mse: 10.0811\n",
      "Epoch 22/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.8615 - mse: 9.8615\n",
      "Epoch 23/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.3277 - mse: 10.3277\n",
      "Epoch 24/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 10.3049 - mse: 10.3049\n",
      "Epoch 25/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.9392 - mse: 8.9392\n",
      "Epoch 26/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 10.0623 - mse: 10.0623\n",
      "Epoch 27/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.9452 - mse: 8.9452\n",
      "Epoch 28/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.2927 - mse: 9.2927\n",
      "Epoch 29/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.1778 - mse: 8.1778\n",
      "Epoch 30/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.9127 - mse: 8.9127\n",
      "Epoch 31/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.4559 - mse: 9.4559\n",
      "Epoch 32/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.3719 - mse: 7.3719\n",
      "Epoch 33/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.2859 - mse: 9.2859\n",
      "Epoch 34/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.3892 - mse: 9.3892\n",
      "Epoch 35/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1236 - mse: 8.1236\n",
      "Epoch 36/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.3475 - mse: 8.3475\n",
      "Epoch 37/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.4845 - mse: 8.4845\n",
      "Epoch 38/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.7229 - mse: 7.7229\n",
      "Epoch 39/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.1062 - mse: 8.1062\n",
      "Epoch 40/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.4995 - mse: 8.4995\n",
      "Epoch 41/70\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 8.1944 - mse: 8.1944\n",
      "Epoch 42/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.3011 - mse: 8.3011\n",
      "Epoch 43/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.2020 - mse: 8.2020\n",
      "Epoch 44/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.4222 - mse: 7.4222\n",
      "Epoch 45/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.9280 - mse: 7.9280\n",
      "Epoch 46/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.2753 - mse: 8.2753\n",
      "Epoch 47/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.6209 - mse: 8.6209\n",
      "Epoch 48/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 6.6876 - mse: 6.6876\n",
      "Epoch 49/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.9288 - mse: 8.9288\n",
      "Epoch 50/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.7399 - mse: 6.7399\n",
      "Epoch 51/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.3028 - mse: 8.3028\n",
      "Epoch 52/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.8581 - mse: 7.8581\n",
      "Epoch 53/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.2587 - mse: 7.2587\n",
      "Epoch 54/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.3326 - mse: 8.3326\n",
      "Epoch 55/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.1197 - mse: 7.1197\n",
      "Epoch 56/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.0744 - mse: 7.0744\n",
      "Epoch 57/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.0666 - mse: 7.0666\n",
      "Epoch 58/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.3788 - mse: 7.3788\n",
      "Epoch 59/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.6161 - mse: 7.6161\n",
      "Epoch 60/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.0453 - mse: 7.0453\n",
      "Epoch 61/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 6.8557 - mse: 6.8557\n",
      "Epoch 62/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.9256 - mse: 7.9256\n",
      "Epoch 63/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 7.6002 - mse: 7.6002\n",
      "Epoch 64/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.6859 - mse: 6.6859\n",
      "Epoch 65/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.5275 - mse: 7.5275\n",
      "Epoch 66/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.6126 - mse: 6.6126\n",
      "Epoch 67/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.1859 - mse: 7.1859\n",
      "Epoch 68/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.7691 - mse: 6.7691\n",
      "Epoch 69/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.4949 - mse: 7.4949\n",
      "Epoch 70/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.0984 - mse: 6.0984\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2fdf2be50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2fdf2be50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:17:10.860800: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6570841754197109\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338dcedc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338dcedc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:18:21.236777: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 9s 246ms/step - loss: 94.1183 - mse: 94.1183\n",
      "Epoch 2/70\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 20.7041 - mse: 20.7041\n",
      "Epoch 3/70\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 19.7349 - mse: 19.7349\n",
      "Epoch 4/70\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 16.4420 - mse: 16.4420\n",
      "Epoch 5/70\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 13.9390 - mse: 13.9390\n",
      "Epoch 6/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 13.8592 - mse: 13.8592\n",
      "Epoch 7/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 13.5379 - mse: 13.5379\n",
      "Epoch 8/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 12.8431 - mse: 12.8431\n",
      "Epoch 9/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 12.4126 - mse: 12.4126\n",
      "Epoch 10/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 11.6196 - mse: 11.6196\n",
      "Epoch 11/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 10.8612 - mse: 10.8612\n",
      "Epoch 12/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 11.4679 - mse: 11.4679\n",
      "Epoch 13/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 11.1208 - mse: 11.1208\n",
      "Epoch 14/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 11.0168 - mse: 11.0168\n",
      "Epoch 15/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 10.3911 - mse: 10.3911\n",
      "Epoch 16/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.6587 - mse: 10.6587\n",
      "Epoch 17/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.9197 - mse: 9.9197\n",
      "Epoch 18/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.5349 - mse: 9.5349\n",
      "Epoch 19/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 10.0588 - mse: 10.0588\n",
      "Epoch 20/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 10.5089 - mse: 10.5089\n",
      "Epoch 21/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.8996 - mse: 9.8996\n",
      "Epoch 22/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.7299 - mse: 9.7299\n",
      "Epoch 23/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.8634 - mse: 9.8634\n",
      "Epoch 24/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.6428 - mse: 9.6428\n",
      "Epoch 25/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 9.1627 - mse: 9.1627\n",
      "Epoch 26/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.8735 - mse: 7.8735\n",
      "Epoch 27/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.9184 - mse: 9.9184\n",
      "Epoch 28/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.1190 - mse: 9.1190\n",
      "Epoch 29/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.8477 - mse: 9.8477\n",
      "Epoch 30/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 7.9016 - mse: 7.9016\n",
      "Epoch 31/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.3522 - mse: 9.3522\n",
      "Epoch 32/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.5866 - mse: 8.5866\n",
      "Epoch 33/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.2806 - mse: 8.2806\n",
      "Epoch 34/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.6216 - mse: 8.6216\n",
      "Epoch 35/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.7957 - mse: 7.7957\n",
      "Epoch 36/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 9.3506 - mse: 9.3506\n",
      "Epoch 37/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1464 - mse: 8.1464\n",
      "Epoch 38/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.2967 - mse: 8.2967\n",
      "Epoch 39/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.5142 - mse: 8.5142\n",
      "Epoch 40/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.6698 - mse: 7.6698\n",
      "Epoch 41/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.5759 - mse: 8.5759\n",
      "Epoch 42/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.1352 - mse: 8.1352\n",
      "Epoch 43/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.7586 - mse: 7.7586\n",
      "Epoch 44/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.2793 - mse: 7.2793\n",
      "Epoch 45/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.8243 - mse: 7.8243\n",
      "Epoch 46/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.6823 - mse: 8.6823\n",
      "Epoch 47/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.7749 - mse: 7.7749\n",
      "Epoch 48/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 6.5845 - mse: 6.5845\n",
      "Epoch 49/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.4048 - mse: 9.4048\n",
      "Epoch 50/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.4816 - mse: 6.4816\n",
      "Epoch 51/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.6642 - mse: 7.6642\n",
      "Epoch 52/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.9574 - mse: 7.9574\n",
      "Epoch 53/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.0893 - mse: 7.0893\n",
      "Epoch 54/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.0295 - mse: 8.0295\n",
      "Epoch 55/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 6.9501 - mse: 6.9501\n",
      "Epoch 56/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.9654 - mse: 7.9654\n",
      "Epoch 57/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.1529 - mse: 7.1529\n",
      "Epoch 58/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.2521 - mse: 8.2521\n",
      "Epoch 59/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 6.6624 - mse: 6.6624\n",
      "Epoch 60/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.5746 - mse: 7.5746\n",
      "Epoch 61/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.5192 - mse: 7.5192\n",
      "Epoch 62/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 6.9753 - mse: 6.9753\n",
      "Epoch 63/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.2300 - mse: 7.2300\n",
      "Epoch 64/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 5.8546 - mse: 5.8546\n",
      "Epoch 65/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1825 - mse: 8.1825\n",
      "Epoch 66/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.9418 - mse: 6.9418\n",
      "Epoch 67/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.2545 - mse: 7.2545\n",
      "Epoch 68/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.1567 - mse: 7.1567\n",
      "Epoch 69/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.6022 - mse: 7.6022\n",
      "Epoch 70/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.1405 - mse: 6.1405\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b4f4fc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b4f4fc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:19:16.384922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.735161056268921\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab3788b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab3788b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:20:27.397526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 264ms/step - loss: 88.5979 - mse: 88.5979\n",
      "Epoch 2/70\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 20.9631 - mse: 20.9631\n",
      "Epoch 3/70\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 19.4434 - mse: 19.4434\n",
      "Epoch 4/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 16.4576 - mse: 16.4576\n",
      "Epoch 5/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 13.6364 - mse: 13.6364\n",
      "Epoch 6/70\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 14.6971 - mse: 14.6971\n",
      "Epoch 7/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 13.8810 - mse: 13.8810\n",
      "Epoch 8/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 11.6680 - mse: 11.6680\n",
      "Epoch 9/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 11.8700 - mse: 11.8700\n",
      "Epoch 10/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 13.1839 - mse: 13.1839\n",
      "Epoch 11/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 12.7377 - mse: 12.7377\n",
      "Epoch 12/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 11.1675 - mse: 11.1675\n",
      "Epoch 13/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 11.8829 - mse: 11.8829\n",
      "Epoch 14/70\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 10.7485 - mse: 10.7485\n",
      "Epoch 15/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 11.0620 - mse: 11.0620\n",
      "Epoch 16/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 11.0825 - mse: 11.0825\n",
      "Epoch 17/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 10.9017 - mse: 10.9017\n",
      "Epoch 18/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.6190 - mse: 9.6190\n",
      "Epoch 19/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 11.5681 - mse: 11.5681\n",
      "Epoch 20/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.1643 - mse: 9.1643\n",
      "Epoch 21/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 10.8073 - mse: 10.8073\n",
      "Epoch 22/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.7344 - mse: 9.7344\n",
      "Epoch 23/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 10.0847 - mse: 10.0847\n",
      "Epoch 24/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.9906 - mse: 8.9906\n",
      "Epoch 25/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.6463 - mse: 9.6463\n",
      "Epoch 26/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 9.1523 - mse: 9.1523\n",
      "Epoch 27/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 9.9470 - mse: 9.9470\n",
      "Epoch 28/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.6724 - mse: 9.6724\n",
      "Epoch 29/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.0222 - mse: 9.0222\n",
      "Epoch 30/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.9124 - mse: 8.9124\n",
      "Epoch 31/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.0747 - mse: 8.0747\n",
      "Epoch 32/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.4013 - mse: 9.4013\n",
      "Epoch 33/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.5459 - mse: 8.5459\n",
      "Epoch 34/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.6262 - mse: 9.6262\n",
      "Epoch 35/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.7218 - mse: 8.7218\n",
      "Epoch 36/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.9625 - mse: 8.9625\n",
      "Epoch 37/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.3974 - mse: 8.3974\n",
      "Epoch 38/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.4692 - mse: 8.4692\n",
      "Epoch 39/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.5556 - mse: 8.5556\n",
      "Epoch 40/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.8891 - mse: 8.8891\n",
      "Epoch 41/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1496 - mse: 8.1496\n",
      "Epoch 42/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.2704 - mse: 8.2704\n",
      "Epoch 43/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.8979 - mse: 8.8979\n",
      "Epoch 44/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.6540 - mse: 7.6540\n",
      "Epoch 45/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.8059 - mse: 8.8059\n",
      "Epoch 46/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.6856 - mse: 7.6856\n",
      "Epoch 47/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.6694 - mse: 7.6694\n",
      "Epoch 48/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.0155 - mse: 8.0155\n",
      "Epoch 49/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.0324 - mse: 8.0324\n",
      "Epoch 50/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.6461 - mse: 7.6461\n",
      "Epoch 51/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.0426 - mse: 7.0426\n",
      "Epoch 52/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.1143 - mse: 8.1143\n",
      "Epoch 53/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.5329 - mse: 7.5329\n",
      "Epoch 54/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.6056 - mse: 7.6056\n",
      "Epoch 55/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.9206 - mse: 7.9206\n",
      "Epoch 56/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 6.8813 - mse: 6.8813\n",
      "Epoch 57/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 7.3911 - mse: 7.3911\n",
      "Epoch 58/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4514 - mse: 7.4514\n",
      "Epoch 59/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 6.7608 - mse: 6.7608\n",
      "Epoch 60/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 6.8239 - mse: 6.8239\n",
      "Epoch 61/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.0954 - mse: 7.0954\n",
      "Epoch 62/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 7.2109 - mse: 7.2109\n",
      "Epoch 63/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.6638 - mse: 7.6638\n",
      "Epoch 64/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 7.5418 - mse: 7.5418\n",
      "Epoch 65/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4862 - mse: 7.4862\n",
      "Epoch 66/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.1905 - mse: 6.1905\n",
      "Epoch 67/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4860 - mse: 7.4860\n",
      "Epoch 68/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 6.0722 - mse: 6.0722\n",
      "Epoch 69/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.2972 - mse: 7.2972\n",
      "Epoch 70/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 6.3558 - mse: 6.3558\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b204faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b204faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:21:21.886935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7731612570108897\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338675ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x338675ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:22:28.496565: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 9s 275ms/step - loss: 96.2351 - mse: 96.2351\n",
      "Epoch 2/70\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 19.2980 - mse: 19.2980\n",
      "Epoch 3/70\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 19.8105 - mse: 19.8105\n",
      "Epoch 4/70\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 15.2264 - mse: 15.2264\n",
      "Epoch 5/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 14.8000 - mse: 14.8000\n",
      "Epoch 6/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 11.7204 - mse: 11.7204\n",
      "Epoch 7/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 12.5334 - mse: 12.5334\n",
      "Epoch 8/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 13.1821 - mse: 13.1821\n",
      "Epoch 9/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 13.3531 - mse: 13.3531\n",
      "Epoch 10/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 11.5591 - mse: 11.5591\n",
      "Epoch 11/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 11.0718 - mse: 11.0718\n",
      "Epoch 12/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.7981 - mse: 9.7981\n",
      "Epoch 13/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 11.4540 - mse: 11.4540\n",
      "Epoch 14/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 10.9819 - mse: 10.9819\n",
      "Epoch 15/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 11.8786 - mse: 11.8786\n",
      "Epoch 16/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 9.7720 - mse: 9.7720\n",
      "Epoch 17/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 11.1952 - mse: 11.1952\n",
      "Epoch 18/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.7300 - mse: 9.7300\n",
      "Epoch 19/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 10.5246 - mse: 10.5246\n",
      "Epoch 20/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.8085 - mse: 9.8085\n",
      "Epoch 21/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 10.2815 - mse: 10.2815\n",
      "Epoch 22/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.7606 - mse: 9.7606\n",
      "Epoch 23/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.4682 - mse: 9.4682\n",
      "Epoch 24/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.7856 - mse: 9.7856\n",
      "Epoch 25/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.5830 - mse: 9.5830\n",
      "Epoch 26/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.6842 - mse: 9.6842\n",
      "Epoch 27/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.1489 - mse: 8.1489\n",
      "Epoch 28/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.2234 - mse: 9.2234\n",
      "Epoch 29/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.3101 - mse: 9.3101\n",
      "Epoch 30/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.3855 - mse: 9.3855\n",
      "Epoch 31/70\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 7.8166 - mse: 7.8166\n",
      "Epoch 32/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 9.3356 - mse: 9.3356\n",
      "Epoch 33/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.1383 - mse: 9.1383\n",
      "Epoch 34/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 7.7709 - mse: 7.7709\n",
      "Epoch 35/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.6872 - mse: 8.6872\n",
      "Epoch 36/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 9.0992 - mse: 9.0992\n",
      "Epoch 37/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 8.5445 - mse: 8.5445\n",
      "Epoch 38/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 7.6424 - mse: 7.6424\n",
      "Epoch 39/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 9.0515 - mse: 9.0515\n",
      "Epoch 40/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.6178 - mse: 8.6178\n",
      "Epoch 41/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 8.5658 - mse: 8.5658\n",
      "Epoch 42/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.9602 - mse: 7.9602\n",
      "Epoch 43/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 7.3569 - mse: 7.3569\n",
      "Epoch 44/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.2067 - mse: 9.2067\n",
      "Epoch 45/70\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 7.6664 - mse: 7.6664\n",
      "Epoch 46/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.9951 - mse: 7.9951\n",
      "Epoch 47/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.6938 - mse: 7.6938\n",
      "Epoch 48/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1927 - mse: 8.1927\n",
      "Epoch 49/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 6.8691 - mse: 6.8691\n",
      "Epoch 50/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.3017 - mse: 8.3017\n",
      "Epoch 51/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 6.6415 - mse: 6.6415\n",
      "Epoch 52/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 8.4787 - mse: 8.4787\n",
      "Epoch 53/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.0074 - mse: 7.0074\n",
      "Epoch 54/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.2176 - mse: 7.2176\n",
      "Epoch 55/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 7.8977 - mse: 7.8977\n",
      "Epoch 56/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.0761 - mse: 7.0761\n",
      "Epoch 57/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.6667 - mse: 7.6667\n",
      "Epoch 58/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.3048 - mse: 7.3048\n",
      "Epoch 59/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.7668 - mse: 7.7668\n",
      "Epoch 60/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.3165 - mse: 7.3165\n",
      "Epoch 61/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 6.8659 - mse: 6.8659\n",
      "Epoch 62/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.2865 - mse: 7.2865\n",
      "Epoch 63/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 6.9884 - mse: 6.9884\n",
      "Epoch 64/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.8982 - mse: 6.8982\n",
      "Epoch 65/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 7.1251 - mse: 7.1251\n",
      "Epoch 66/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 6.8831 - mse: 6.8831\n",
      "Epoch 67/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.3932 - mse: 7.3932\n",
      "Epoch 68/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 6.7858 - mse: 6.7858\n",
      "Epoch 69/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 6.1918 - mse: 6.1918\n",
      "Epoch 70/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.9092 - mse: 6.9092\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2fe5681f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2fe5681f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:23:24.468985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.7634403930408005\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3396e9940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3396e9940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:24:34.313024: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 281ms/step - loss: 91.3823 - mse: 91.3823\n",
      "Epoch 2/70\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 21.3884 - mse: 21.3884\n",
      "Epoch 3/70\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 18.8363 - mse: 18.8363\n",
      "Epoch 4/70\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 13.8527 - mse: 13.8527\n",
      "Epoch 5/70\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 14.5740 - mse: 14.5740\n",
      "Epoch 6/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 14.2227 - mse: 14.2227\n",
      "Epoch 7/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 11.4148 - mse: 11.4148\n",
      "Epoch 8/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 12.8368 - mse: 12.8368\n",
      "Epoch 9/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 14.1845 - mse: 14.1845\n",
      "Epoch 10/70\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 9.4259 - mse: 9.4259\n",
      "Epoch 11/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 13.2310 - mse: 13.2310\n",
      "Epoch 12/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 11.3770 - mse: 11.3770\n",
      "Epoch 13/70\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 10.8221 - mse: 10.8221\n",
      "Epoch 14/70\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 11.7400 - mse: 11.7400\n",
      "Epoch 15/70\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 11.3180 - mse: 11.3180\n",
      "Epoch 16/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.6896 - mse: 9.6896\n",
      "Epoch 17/70\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 11.9232 - mse: 11.9232\n",
      "Epoch 18/70\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 8.8194 - mse: 8.8194\n",
      "Epoch 19/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 11.2644 - mse: 11.2644\n",
      "Epoch 20/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 9.8446 - mse: 9.8446\n",
      "Epoch 21/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 10.3308 - mse: 10.3308\n",
      "Epoch 22/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 10.8165 - mse: 10.8165\n",
      "Epoch 23/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.4457 - mse: 9.4457\n",
      "Epoch 24/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.7697 - mse: 9.7697\n",
      "Epoch 25/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.8413 - mse: 8.8413\n",
      "Epoch 26/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 9.3900 - mse: 9.3900\n",
      "Epoch 27/70\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 9.3325 - mse: 9.3325\n",
      "Epoch 28/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.7366 - mse: 9.7366\n",
      "Epoch 29/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.9969 - mse: 9.9969\n",
      "Epoch 30/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.1641 - mse: 8.1641\n",
      "Epoch 31/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.5886 - mse: 9.5886\n",
      "Epoch 32/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.8756 - mse: 7.8756\n",
      "Epoch 33/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.2309 - mse: 9.2309\n",
      "Epoch 34/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 9.3611 - mse: 9.3611\n",
      "Epoch 35/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.0632 - mse: 8.0632\n",
      "Epoch 36/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 9.1695 - mse: 9.1695\n",
      "Epoch 37/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.3787 - mse: 8.3787\n",
      "Epoch 38/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.0543 - mse: 8.0543\n",
      "Epoch 39/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 9.0709 - mse: 9.0709\n",
      "Epoch 40/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.7287 - mse: 8.7287\n",
      "Epoch 41/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.5168 - mse: 8.5168\n",
      "Epoch 42/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.1515 - mse: 8.1515\n",
      "Epoch 43/70\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 8.1404 - mse: 8.1404\n",
      "Epoch 44/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.2760 - mse: 7.2760\n",
      "Epoch 45/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.5582 - mse: 8.5582\n",
      "Epoch 46/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.7978 - mse: 7.7978\n",
      "Epoch 47/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.9455 - mse: 7.9455\n",
      "Epoch 48/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4725 - mse: 7.4725\n",
      "Epoch 49/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 8.4901 - mse: 8.4901\n",
      "Epoch 50/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.3881 - mse: 7.3881\n",
      "Epoch 51/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 8.0400 - mse: 8.0400\n",
      "Epoch 52/70\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 8.0505 - mse: 8.0505\n",
      "Epoch 53/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.0786 - mse: 7.0786\n",
      "Epoch 54/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.6363 - mse: 7.6363\n",
      "Epoch 55/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.9737 - mse: 7.9737\n",
      "Epoch 56/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 7.7094 - mse: 7.7094\n",
      "Epoch 57/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.5892 - mse: 7.5892\n",
      "Epoch 58/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.0049 - mse: 7.0049\n",
      "Epoch 59/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4482 - mse: 7.4482\n",
      "Epoch 60/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.4644 - mse: 7.4644\n",
      "Epoch 61/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 7.2504 - mse: 7.2504\n",
      "Epoch 62/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.6141 - mse: 7.6141\n",
      "Epoch 63/70\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 6.3418 - mse: 6.3418\n",
      "Epoch 64/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 8.0160 - mse: 8.0160\n",
      "Epoch 65/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.2862 - mse: 6.2862\n",
      "Epoch 66/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 7.4012 - mse: 7.4012\n",
      "Epoch 67/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.8390 - mse: 6.8390\n",
      "Epoch 68/70\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 7.0251 - mse: 7.0251\n",
      "Epoch 69/70\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 6.3184 - mse: 6.3184\n",
      "Epoch 70/70\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 6.4878 - mse: 6.4878\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e482ea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2e482ea60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:25:29.931194: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6180189616382972\n",
      "Average kappa score value is : 0.7093731686757239\n",
      "\n",
      "--------SET 8--------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x177395940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x177395940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:26:12.543638: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 410ms/step - loss: 1095.2480 - mse: 1095.2480\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 207.3703 - mse: 207.3703\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 47.9961 - mse: 47.9961\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 41.7506 - mse: 41.7506\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 40.1062 - mse: 40.1062\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 38.8780 - mse: 38.8780\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 39.0307 - mse: 39.0307\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 40.8740 - mse: 40.8740\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 37.6714 - mse: 37.6714\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 33.5329 - mse: 33.5329\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 37.7718 - mse: 37.7718\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 33.1248 - mse: 33.1248\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 28.7528 - mse: 28.7528\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 28.0210 - mse: 28.0210\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 25.5305 - mse: 25.5305\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 29.8831 - mse: 29.8831\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 23.7308 - mse: 23.7308\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 24.5735 - mse: 24.5735\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 26.5391 - mse: 26.5391\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 20.7665 - mse: 20.7665\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 27.0461 - mse: 27.0461\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 27.0803 - mse: 27.0803\n",
      "Epoch 23/70\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 21.0076 - mse: 21.0076\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 23.0156 - mse: 23.0156\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 24.1041 - mse: 24.1041\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 19.2366 - mse: 19.2366\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 24.9214 - mse: 24.9214\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.9333 - mse: 21.9333\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 22.8171 - mse: 22.8171\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 21.1983 - mse: 21.1983\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 22.8835 - mse: 22.8835\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 18.0103 - mse: 18.0103\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.4763 - mse: 19.4763\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 24.4345 - mse: 24.4345\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 20.8575 - mse: 20.8575\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 21.7459 - mse: 21.7459\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 18.6015 - mse: 18.6015\n",
      "Epoch 38/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 20.2581 - mse: 20.2581\n",
      "Epoch 39/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 18.4672 - mse: 18.4672\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 15.3065 - mse: 15.3065\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 29.0382 - mse: 29.0382\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 17.9926 - mse: 17.9926\n",
      "Epoch 43/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 20.3354 - mse: 20.3354\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.2782 - mse: 18.2782\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 24.2216 - mse: 24.2216\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 16.1476 - mse: 16.1476\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 20.2901 - mse: 20.2901\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 19.1014 - mse: 19.1014\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.3734 - mse: 18.3734\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 19.5220 - mse: 19.5220\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 17.1827 - mse: 17.1827\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 17.6191 - mse: 17.6191\n",
      "Epoch 53/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 20.1354 - mse: 20.1354\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 14.4856 - mse: 14.4856\n",
      "Epoch 55/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 23.4164 - mse: 23.4164\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 16.4675 - mse: 16.4675\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 15.9501 - mse: 15.9501\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 17.9794 - mse: 17.9794\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 17.6659 - mse: 17.6659\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 18.0873 - mse: 18.0873\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 14.2816 - mse: 14.2816\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 18.5932 - mse: 18.5932\n",
      "Epoch 63/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 17.7274 - mse: 17.7274\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 17.5953 - mse: 17.5953\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 19.0751 - mse: 19.0751\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 16.3196 - mse: 16.3196\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 17.5756 - mse: 17.5756\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 20.7024 - mse: 20.7024\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 13.4587 - mse: 13.4587\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 16.7198 - mse: 16.7198\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2097dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b2097dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:26:44.596429: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.4863741314937511\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b1f3c9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b1f3c9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:27:15.646628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 351ms/step - loss: 1008.9413 - mse: 1008.9413\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 94.9053 - mse: 94.9053\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 40.1989 - mse: 40.1989\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 40.9319 - mse: 40.9319\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 39.5324 - mse: 39.5324\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 39.5287 - mse: 39.5287\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 38.9667 - mse: 38.9667\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 36.9451 - mse: 36.9451\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 41.1389 - mse: 41.1389\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 33.5160 - mse: 33.5160\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 25.4169 - mse: 25.4169\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 38.0575 - mse: 38.0575\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 25.6265 - mse: 25.6265\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 33.3063 - mse: 33.3063\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 24.6961 - mse: 24.6961\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 32.5316 - mse: 32.5316\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 25.7382 - mse: 25.7382\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 28.0490 - mse: 28.0490\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 20.2459 - mse: 20.2459\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 38.7360 - mse: 38.7360\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 18.8157 - mse: 18.8157\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 22.8305 - mse: 22.8305\n",
      "Epoch 23/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 26.3945 - mse: 26.3945\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 24.4147 - mse: 24.4147\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 18.8540 - mse: 18.8540\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 19.1375 - mse: 19.1375\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 26.8333 - mse: 26.8333\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 25.6616 - mse: 25.6616\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 17.4263 - mse: 17.4263\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 28.7404 - mse: 28.7404\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 21.6720 - mse: 21.6720\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 16.5613 - mse: 16.5613\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 17.1671 - mse: 17.1671\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 28.4079 - mse: 28.4079\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 18.3111 - mse: 18.3111\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 25.4811 - mse: 25.4811\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 20.7244 - mse: 20.7244\n",
      "Epoch 38/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 17.6824 - mse: 17.6824\n",
      "Epoch 39/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 16.7410 - mse: 16.7410\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 26.3950 - mse: 26.3950\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 21.7344 - mse: 21.7344\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 18.3043 - mse: 18.3043\n",
      "Epoch 43/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 24.4613 - mse: 24.4613\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 15.4231 - mse: 15.4231\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 22.6166 - mse: 22.6166\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.1387 - mse: 18.1387\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 19.1307 - mse: 19.1307\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 22.3710 - mse: 22.3710\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 14.9072 - mse: 14.9072\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 24.1511 - mse: 24.1511\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 15.4774 - mse: 15.4774\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.7367 - mse: 18.7367\n",
      "Epoch 53/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 22.4828 - mse: 22.4828\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 19.4867 - mse: 19.4867\n",
      "Epoch 55/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 17.7938 - mse: 17.7938\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 20.4781 - mse: 20.4781\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 18.5766 - mse: 18.5766\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 16.9567 - mse: 16.9567\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 19.8108 - mse: 19.8108\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 15.3401 - mse: 15.3401\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 18.3511 - mse: 18.3511\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - 0s 69ms/step - loss: 18.1823 - mse: 18.1823\n",
      "Epoch 63/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 18.6186 - mse: 18.6186\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.7398 - mse: 18.7398\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 15.7441 - mse: 15.7441\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 20.9684 - mse: 20.9684\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 15.8277 - mse: 15.8277\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 14.2720 - mse: 14.2720\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 23.3775 - mse: 23.3775\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 16.1657 - mse: 16.1657\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3b646a040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x3b646a040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:27:46.834221: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.6478597850386354\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9dfde50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9dfde50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:28:23.174467: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 386ms/step - loss: 1136.8839 - mse: 1136.8839\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 253.7674 - mse: 253.7674\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 50.5942 - mse: 50.5942\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 37.1948 - mse: 37.1948\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 36.8110 - mse: 36.8110\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - 1s 95ms/step - loss: 36.5600 - mse: 36.5600\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 37.9823 - mse: 37.9823\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 38.9233 - mse: 38.9233\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 37.6842 - mse: 37.6842\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 37.7527 - mse: 37.7527\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 43.4095 - mse: 43.4095\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 29.0148 - mse: 29.0148\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 25.2215 - mse: 25.2215\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 34.4727 - mse: 34.4727\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 25.6230 - mse: 25.6230\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 28.5657 - mse: 28.5657\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 28.9478 - mse: 28.9478\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 24.9150 - mse: 24.9150\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 19.6834 - mse: 19.6834\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 30.5780 - mse: 30.5780\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 20.5458 - mse: 20.5458\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 22.9296 - mse: 22.9296\n",
      "Epoch 23/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 21.6171 - mse: 21.6171\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 24.3676 - mse: 24.3676\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 23.9439 - mse: 23.9439\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 26.4021 - mse: 26.4021\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 19.8922 - mse: 19.8922\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 26.1144 - mse: 26.1144\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 20.6886 - mse: 20.6886\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.9774 - mse: 21.9774\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 21.7420 - mse: 21.7420\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 21.5278 - mse: 21.5278\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 22.6695 - mse: 22.6695\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 20.3062 - mse: 20.3062\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 23.6271 - mse: 23.6271\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 18.7646 - mse: 18.7646\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 16.6738 - mse: 16.6738\n",
      "Epoch 38/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 23.7222 - mse: 23.7222\n",
      "Epoch 39/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 17.1954 - mse: 17.1954\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 29.5010 - mse: 29.5010\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 17.3473 - mse: 17.3473\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 19.0210 - mse: 19.0210\n",
      "Epoch 43/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 21.0573 - mse: 21.0573\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 17.7079 - mse: 17.7079\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 22.0363 - mse: 22.0363\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 18.0628 - mse: 18.0628\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 21.8532 - mse: 21.8532\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 21.6208 - mse: 21.6208\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - 1s 97ms/step - loss: 16.7796 - mse: 16.7796\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 21.1278 - mse: 21.1278\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 17.7998 - mse: 17.7998\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 17.0775 - mse: 17.0775\n",
      "Epoch 53/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.4463 - mse: 21.4463\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 16.4287 - mse: 16.4287\n",
      "Epoch 55/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 21.5914 - mse: 21.5914\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 18.3216 - mse: 18.3216\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 16.8640 - mse: 16.8640\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 19.9725 - mse: 19.9725\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 16.6125 - mse: 16.6125\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 18.6995 - mse: 18.6995\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 16.9561 - mse: 16.9561\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 14.4815 - mse: 14.4815\n",
      "Epoch 63/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 20.0420 - mse: 20.0420\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 16.2232 - mse: 16.2232\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 18.6470 - mse: 18.6470\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 15.4969 - mse: 15.4969\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 17.4542 - mse: 17.4542\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 22.0246 - mse: 22.0246\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 14.3700 - mse: 14.3700\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 19.5415 - mse: 19.5415\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b5207ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b5207ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:28:57.229855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5361318480018098\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x305b1bb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x305b1bb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:29:29.194221: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 391ms/step - loss: 1009.2422 - mse: 1009.2422\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 96.6880 - mse: 96.6880\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 42.4050 - mse: 42.4050\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 40.1611 - mse: 40.1611\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 39.4338 - mse: 39.4338\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 36.9067 - mse: 36.9067\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 38.0164 - mse: 38.0164\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 41.4207 - mse: 41.4207\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 31.9287 - mse: 31.9287\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 30.6424 - mse: 30.6424\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 33.2402 - mse: 33.2402\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 30.2329 - mse: 30.2329\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 23.2229 - mse: 23.2229\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 33.9663 - mse: 33.9663\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 22.8922 - mse: 22.8923\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 24.6698 - mse: 24.6698\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 35.7235 - mse: 35.7235\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 27.3832 - mse: 27.3832\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 23.4024 - mse: 23.4024\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 28.9657 - mse: 28.9657\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 18.6651 - mse: 18.6651\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 29.3238 - mse: 29.3238\n",
      "Epoch 23/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 20.9457 - mse: 20.9457\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 29.7004 - mse: 29.7004\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 23.7539 - mse: 23.7539\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 25.2380 - mse: 25.2380\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.7565 - mse: 19.7565\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 24.7406 - mse: 24.7406\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 17.7566 - mse: 17.7566\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 26.4414 - mse: 26.4414\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 22.3931 - mse: 22.3931\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 20.4062 - mse: 20.4062\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 24.7924 - mse: 24.7924\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 22.6758 - mse: 22.6758\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.9174 - mse: 19.9174\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 20.9122 - mse: 20.9122\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 21.8576 - mse: 21.8576\n",
      "Epoch 38/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.6000 - mse: 19.6000\n",
      "Epoch 39/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 22.4390 - mse: 22.4390\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 22.7498 - mse: 22.7498\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 22.2908 - mse: 22.2908\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.7706 - mse: 19.7706\n",
      "Epoch 43/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 17.0440 - mse: 17.0440\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 26.8423 - mse: 26.8423\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 21.6565 - mse: 21.6565\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 16.6741 - mse: 16.6741\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 17.4414 - mse: 17.4414\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 19.5901 - mse: 19.5901\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 20.9520 - mse: 20.9520\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.5428 - mse: 19.5428\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 18.6889 - mse: 18.6889\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 21.0573 - mse: 21.0573\n",
      "Epoch 53/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 16.0339 - mse: 16.0339\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 22.1636 - mse: 22.1636\n",
      "Epoch 55/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 15.1486 - mse: 15.1486\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 20.9983 - mse: 20.9983\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 17.4046 - mse: 17.4046\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 18.1961 - mse: 18.1961\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.9272 - mse: 19.9272\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 19.3393 - mse: 19.3393\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 18.2868 - mse: 18.2868\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 17.2824 - mse: 17.2824\n",
      "Epoch 63/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 18.4131 - mse: 18.4131\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 19.6470 - mse: 19.6470\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 14.5734 - mse: 14.5734\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 20.0448 - mse: 20.0448\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 15.3012 - mse: 15.3012\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 21.9596 - mse: 21.9596\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 14.2523 - mse: 14.2523\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 14.0808 - mse: 14.0808\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b52409d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2b52409d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:30:00.790711: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5507206263221368\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 800)           3740800   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              951296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,710,657\n",
      "Trainable params: 4,710,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa92f5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa92f5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:30:33.534554: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 8s 352ms/step - loss: 1058.9618 - mse: 1058.9618\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 174.7779 - mse: 174.7779\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 43.1252 - mse: 43.1252\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 38.6779 - mse: 38.6779\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 38.0746 - mse: 38.0746\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 38.4956 - mse: 38.4956\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 37.7157 - mse: 37.7157\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 35.7541 - mse: 35.7541\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 34.7805 - mse: 34.7805\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 35.5511 - mse: 35.5511\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 31.0828 - mse: 31.0828\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 33.8607 - mse: 33.8607\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 27.7217 - mse: 27.7217\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 30.3853 - mse: 30.3853\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 24.7919 - mse: 24.7919\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 24.2327 - mse: 24.2327\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 30.6406 - mse: 30.6406\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 25.2506 - mse: 25.2506\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 23.2959 - mse: 23.2959\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 28.1475 - mse: 28.1475\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 24.0300 - mse: 24.0300\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 22.6792 - mse: 22.6792\n",
      "Epoch 23/70\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 22.6547 - mse: 22.6547\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.7866 - mse: 21.7866\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 23.4819 - mse: 23.4819\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 26.4462 - mse: 26.4462\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 19.0056 - mse: 19.0056\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 26.6381 - mse: 26.6381\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 18.5426 - mse: 18.5426\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - 0s 71ms/step - loss: 26.6515 - mse: 26.6515\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 20.8391 - mse: 20.8391\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.9393 - mse: 21.9393\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 24.0569 - mse: 24.0569\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 18.2288 - mse: 18.2288\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 21.7226 - mse: 21.7226\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 19.9193 - mse: 19.9193\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 22.3323 - mse: 22.3323\n",
      "Epoch 38/70\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 19.6610 - mse: 19.6610\n",
      "Epoch 39/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 22.1919 - mse: 22.1919\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 19.9119 - mse: 19.9119\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 15.6520 - mse: 15.6520\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 18.6215 - mse: 18.6215\n",
      "Epoch 43/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 19.8315 - mse: 19.8315\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 20.7384 - mse: 20.7384\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 17.4071 - mse: 17.4071\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 21.8307 - mse: 21.8307\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 19.5044 - mse: 19.5044\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 22.6626 - mse: 22.6626\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 16.4210 - mse: 16.4210\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 14.7825 - mse: 14.7825\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 19.3590 - mse: 19.3590\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 18.7289 - mse: 18.7289\n",
      "Epoch 53/70\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 21.7461 - mse: 21.7461\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 14.8527 - mse: 14.8527\n",
      "Epoch 55/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 16.6405 - mse: 16.6405\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 19.8662 - mse: 19.8662\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 23.5274 - mse: 23.5274\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 16.1199 - mse: 16.1199\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 18.0769 - mse: 18.0769\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 18.0407 - mse: 18.0407\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 17.9637 - mse: 17.9637\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 16.5056 - mse: 16.5056\n",
      "Epoch 63/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 15.2474 - mse: 15.2474\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 18.8468 - mse: 18.8468\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 16.4264 - mse: 16.4264\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 22.3106 - mse: 22.3106\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 15.4815 - mse: 15.4815\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 17.7311 - mse: 17.7311\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 20.2443 - mse: 20.2443\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 15.4936 - mse: 15.4936\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x169d628b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x169d628b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 00:31:06.407947: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.5800317030025961\n",
      "Average kappa score value is : 0.5602236187717858\n"
     ]
    }
   ],
   "source": [
    "## Sets experiment BERT\n",
    "import time\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "set_count = 1\n",
    "all_sets_score = []\n",
    "\n",
    "for s in sets:\n",
    "    print(\"\\n--------SET {}--------\\n\".format(set_count))\n",
    "    X = s\n",
    "    y = s['domain1_score']\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    cv_data = cv.split(X)\n",
    "    results = []\n",
    "    prediction_list = []\n",
    "    fold_count =1\n",
    "#     cuda = torch.device('cuda')\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "#     model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    for traincv, testcv in cv_data:\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(fold_count))\n",
    "\n",
    "        # get the train and test from the dataset.\n",
    "        X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
    "        train_essays = X_train['essay']\n",
    "        test_essays = X_test['essay']\n",
    "\n",
    "        sentences = []\n",
    "        tokenize_sentences = []\n",
    "        train_bert_embeddings = []\n",
    "\n",
    "        tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "        tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "\n",
    "        train_features = get_features(tokenized_train)\n",
    "        test_features = get_features(tokenized_test)\n",
    "\n",
    "        train_x, train_y = train_features.shape\n",
    "        test_x, test_y = test_features.shape\n",
    "\n",
    "        trainDataVectors = np.reshape(train_features,(train_x, 1, train_y))\n",
    "        testDataVectors = np.reshape(test_features,(test_x, 1, test_y))\n",
    "\n",
    "        # Bidirectional LSTM Model\n",
    "        bi_dir_lstm_model = get_model(bidirectional=True)\n",
    "        bi_dir_lstm_model.fit(trainDataVectors, y_train, batch_size=128, epochs=70)\n",
    "        y_pred = bi_dir_lstm_model.predict(testDataVectors)\n",
    "        y_pred = np.around(y_pred)\n",
    "        #y_pred.dropna()\n",
    "        np.nan_to_num(y_pred)\n",
    "\n",
    "        # evaluate the model\n",
    "        result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "        print(\"Kappa Score: {}\".format(result))\n",
    "        results.append(result)\n",
    "        fold_count +=1\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        all_sets_score.append(results)\n",
    "\n",
    "    print(\"Average kappa score value is : {}\".format(np.mean(np.asarray(results))))\n",
    "    set_count+=1\n",
    "        # print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0       1.0        1.0  Dear local newspaper, I think effects computer...   \n",
       "\n",
       "   domain1_score  \n",
       "0            0.6  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.23051694e-02,  1.60252471e-02,  2.04541191e-01,\n",
       "         6.72682151e-02, -1.38523638e-01, -3.21713865e-01,\n",
       "         2.45416909e-01,  7.12756455e-01, -3.32589209e-01,\n",
       "        -3.69510531e-01,  4.78828922e-02, -1.46845967e-01,\n",
       "         6.97909892e-02,  3.53184342e-01, -5.41524887e-02,\n",
       "         1.71720386e-01,  2.62986263e-03,  3.73209089e-01,\n",
       "         1.44100904e-01,  1.86410069e-01,  3.93429771e-02,\n",
       "        -2.83513904e-01,  5.07952750e-01,  1.25221327e-01,\n",
       "         2.49996811e-01, -1.34886950e-01,  1.15930252e-01,\n",
       "         1.98495500e-02,  8.59399214e-02, -5.43922931e-02,\n",
       "        -1.36439830e-01,  3.17826182e-01, -1.26200626e-02,\n",
       "        -1.71214953e-01,  3.97089310e-02, -2.87185997e-01,\n",
       "         9.30588320e-03, -8.81661028e-02,  2.33654715e-02,\n",
       "         1.13749534e-01, -1.58142969e-01,  1.36231363e-01,\n",
       "        -3.46561745e-02, -1.13332152e-01, -7.20796436e-02,\n",
       "        -2.45297521e-01, -2.73655725e+00,  7.36569241e-02,\n",
       "         8.46555829e-03, -1.23966180e-01,  1.31956488e-01,\n",
       "        -8.30303356e-02, -3.10121000e-01,  1.42923862e-01,\n",
       "         3.13570797e-01,  3.82341653e-01, -2.69608885e-01,\n",
       "         1.52935892e-01, -2.33547762e-01, -2.59965137e-02,\n",
       "         3.18816423e-01, -1.16507322e-01, -3.73019308e-01,\n",
       "        -1.67867132e-02, -5.12574762e-02, -2.71753073e-01,\n",
       "         2.36418098e-03,  2.34618962e-01, -4.62031871e-01,\n",
       "         4.74723846e-01, -3.07087243e-01, -3.84515584e-01,\n",
       "         1.20458461e-01, -9.72450972e-02, -3.34781297e-02,\n",
       "        -1.78876758e-01, -6.33831620e-02,  1.82577193e-01,\n",
       "        -4.44471836e-04, -3.72388996e-02,  1.09780692e-01,\n",
       "         3.78717124e-01,  3.08753252e-01, -1.34158507e-02,\n",
       "         9.83100086e-02,  2.38168210e-01, -3.69487584e-01,\n",
       "        -4.09750864e-02,  1.53348804e-01,  2.81833529e-01,\n",
       "         1.96173355e-01, -1.36281461e-01,  1.46407276e-01,\n",
       "         2.17178255e-01,  5.98183870e-01, -1.57649517e-01,\n",
       "         1.89811230e-01, -1.32866994e-01,  9.89254415e-02,\n",
       "         2.40106806e-01,  4.12572294e-01, -3.39174658e-01,\n",
       "         2.31353134e-01, -6.12366021e-01,  1.50643200e-01,\n",
       "        -9.73874331e-02,  2.21468836e-01, -4.44045037e-01,\n",
       "         2.62114525e-01, -1.62232757e+00,  1.80950820e-01,\n",
       "         2.11643040e-01, -5.59719019e-02, -3.39304715e-01,\n",
       "        -1.71031922e-01,  1.71436131e-01,  3.70404184e-01,\n",
       "         1.51771000e-02,  3.56183469e-01,  2.22157374e-01,\n",
       "        -2.12198272e-01,  4.01178360e-01, -9.67152708e-04,\n",
       "        -2.03132853e-02,  1.90468710e-02,  4.89564717e-01,\n",
       "        -2.15202183e-01, -3.56686831e-01,  1.18265592e-01,\n",
       "         2.38771975e-01,  5.04680514e-01,  3.07621688e-01,\n",
       "         6.48745075e-02, -2.46256649e-01,  2.84648985e-02,\n",
       "         3.46652418e-01,  2.08636850e-01, -2.09165052e-01,\n",
       "        -1.38844669e-01,  2.90404502e-02, -1.98486045e-01,\n",
       "        -2.36169085e-01, -2.14186144e+00,  3.96401942e-01,\n",
       "         5.93761981e-01,  1.62368223e-01, -2.49656886e-01,\n",
       "         1.50527090e-01, -9.43607539e-02,  1.55911759e-01,\n",
       "        -6.18506223e-03, -9.34415013e-02, -1.95544869e-01,\n",
       "         9.02366042e-02, -2.09112376e-01,  1.63019344e-01,\n",
       "        -6.16123438e-01, -7.24339485e-02,  1.85322285e-01,\n",
       "         4.39929157e-01,  2.52489984e-01, -1.56951711e-01,\n",
       "        -1.30799696e-01,  1.52490318e-01, -1.53522253e-01,\n",
       "         2.19773836e-02,  2.98379153e-01,  1.39494687e-01,\n",
       "         4.34091091e-02, -1.41920030e-01, -1.37929618e-01,\n",
       "         1.46136060e-01,  5.21384537e-01, -2.02652127e-01,\n",
       "         3.41980070e-01, -1.91913083e-01,  1.36028633e-01,\n",
       "         1.34756193e-01,  7.82107115e-02, -2.60974355e-02,\n",
       "        -2.87967503e-01,  3.36770922e-01, -2.07620561e-01,\n",
       "         5.13393097e-02, -1.78938713e-02, -1.22239754e-01,\n",
       "         3.31994474e-01, -8.30143765e-02, -2.57584453e-02,\n",
       "         1.92629278e-01, -1.59272969e-01, -2.62476206e-01,\n",
       "         5.43182679e-02, -1.07995711e-01,  6.33111656e-01,\n",
       "        -1.84833646e-01,  1.42352313e-01, -3.02153528e-01,\n",
       "         1.40230268e-01,  7.89862424e-02, -1.96155220e-01,\n",
       "        -7.86693096e-02,  7.18003511e-02,  5.56099266e-02,\n",
       "        -2.29384065e-01,  2.91352940e+00,  7.28071406e-02,\n",
       "         3.37177366e-02,  1.56099021e-01,  3.48123163e-01,\n",
       "        -1.74707755e-01,  1.79059148e-01,  4.97536510e-02,\n",
       "        -1.02615319e-01, -9.67604071e-02,  1.78561255e-01,\n",
       "         2.58631021e-01,  9.46994275e-02, -5.15276082e-02,\n",
       "        -1.08140081e-01,  2.97543138e-01, -1.56640977e-01,\n",
       "         1.95624083e-02,  3.19970101e-01, -1.43110976e-02,\n",
       "         1.63797438e-01, -1.60157412e-01,  7.92506933e-02,\n",
       "         2.63451010e-01, -1.25972056e+00,  9.93236750e-02,\n",
       "         5.34604713e-02,  1.70000494e-01,  1.66763648e-01,\n",
       "        -1.54470935e-01, -3.39113139e-02,  1.67038664e-03,\n",
       "        -1.66684180e-01,  1.60936322e-02,  5.31370454e-02,\n",
       "        -2.93517351e-01,  3.58601213e-01,  2.91273117e-01,\n",
       "         1.83334604e-01, -2.33198062e-01,  4.60585386e-01,\n",
       "         2.34457403e-02, -3.18670362e-01,  2.23135740e-01,\n",
       "        -1.94341987e-01,  2.47034192e-01,  3.29943597e-02,\n",
       "         1.38136700e-01, -2.57058144e-01,  2.98797220e-01,\n",
       "        -3.64486575e-02,  1.08372226e-01, -9.53296870e-02,\n",
       "        -3.21083605e-01, -2.55444258e-01, -3.42677176e-01,\n",
       "        -1.72648847e-01, -1.48468278e-02,  1.11860715e-01,\n",
       "        -5.50515652e-01, -1.81270689e-01,  3.35484445e-02,\n",
       "        -3.87952089e-01,  1.00669116e-01, -7.61902630e-02,\n",
       "         1.00044489e-01, -1.70332834e-01, -4.75380808e-01,\n",
       "        -1.94980013e+00,  4.56696786e-02, -2.23860219e-01,\n",
       "         2.69477725e-01,  1.69127539e-01,  7.86560327e-02,\n",
       "         1.09767392e-02, -5.94199300e-02,  4.32229161e-01,\n",
       "        -2.91727215e-01,  4.54665452e-01,  2.53722817e-01,\n",
       "         3.10585164e-02,  2.40102381e-01, -2.42758781e-01,\n",
       "         2.08486646e-01, -7.00495690e-02, -1.28484339e-01,\n",
       "        -1.75047994e-01, -2.46678144e-01, -5.05233966e-02,\n",
       "         4.34234977e-01, -5.49283326e-02,  4.80122954e-01,\n",
       "        -1.43997669e-02, -1.29970551e-01, -1.74396694e-01,\n",
       "        -2.40012243e-01,  1.41613990e-01,  2.49071755e-02,\n",
       "        -1.30617797e-01, -1.91658549e-02, -5.87813091e-03,\n",
       "        -4.10913050e-01, -3.13107550e-01, -4.03507423e+00,\n",
       "        -8.46318752e-02, -5.11136353e-02, -3.55546087e-01,\n",
       "         2.70958357e-02,  5.40404245e-02,  5.67331851e-01,\n",
       "        -1.44114375e-01, -2.23317474e-01, -1.71052516e-02,\n",
       "         1.01431534e-01, -1.54723763e-01,  9.48923081e-02,\n",
       "         1.06183149e-01,  3.38361949e-01,  3.53840321e-01,\n",
       "         6.11243010e-01, -1.71389759e-01,  1.47061735e-01,\n",
       "         3.23402345e-01, -1.35679960e-01, -1.82374120e-01,\n",
       "         7.34175518e-02,  4.93787751e-02,  3.70903194e-01,\n",
       "         2.28439689e-01, -3.02196145e-01, -9.85455960e-02,\n",
       "         5.93755916e-02, -7.59688616e-02,  2.33682722e-01,\n",
       "        -2.38497138e-01, -1.88577965e-01,  1.21609114e-01,\n",
       "        -3.43235016e-01, -7.50841722e-02,  1.30919322e-01,\n",
       "         9.61517468e-02,  3.53672743e-01,  2.93418057e-02,\n",
       "         9.14744735e-02,  4.18909013e-01,  6.23462051e-02,\n",
       "        -7.98551291e-02,  5.88652670e-01, -5.44576123e-02,\n",
       "         2.03846559e-01,  1.17481753e-01, -2.69117355e-01,\n",
       "         2.49639124e-01,  1.53774768e-02, -4.69468161e-03,\n",
       "         1.14428675e+00, -1.29448950e-01, -2.94578243e-02,\n",
       "        -3.47627014e-01,  5.62516212e-01,  3.01623642e-01,\n",
       "        -1.32427201e-01, -8.98881331e-02,  4.40843582e-01,\n",
       "        -2.44727924e-01,  4.87039201e-02, -6.14600062e-01,\n",
       "         9.77419391e-02, -4.04267669e-01,  1.18125051e-01,\n",
       "        -5.87321103e-01,  5.99775389e-02, -3.99154313e-02,\n",
       "        -1.74796835e-01,  3.77579868e-01, -1.26237929e-01,\n",
       "        -8.59438658e-01, -1.78705990e-01, -1.86536461e-02,\n",
       "        -1.45968981e-03,  1.79087847e-01, -1.64090678e-01,\n",
       "         3.84624690e-01, -3.57288867e-01, -1.03436492e-01,\n",
       "        -4.86271620e-01,  3.30486029e-01, -4.50298697e-01,\n",
       "        -2.68484771e-01, -7.98926204e-02, -4.14620340e-03,\n",
       "        -5.57101309e-01,  2.29254365e-02,  1.31901205e-01,\n",
       "         2.24254057e-01,  2.28035808e-01,  2.95702755e-01,\n",
       "         9.85294282e-02,  1.27821982e-01,  2.32414544e-01,\n",
       "        -7.62688816e-01,  1.95444316e-01, -1.75664812e-01,\n",
       "         9.18297991e-02,  2.37701237e-01, -1.16722018e-01,\n",
       "        -6.52970523e-02, -1.35941952e-01, -1.04867704e-01,\n",
       "        -1.03329718e-02,  1.06105700e-01, -3.39232087e-02,\n",
       "        -9.07766223e-02, -2.15685576e-01, -1.04536645e-01,\n",
       "        -3.59945357e-01,  2.72536546e-01,  8.06020677e-01,\n",
       "        -1.51207864e-01,  1.48913302e-02,  2.21948475e-01,\n",
       "         4.90288958e-02,  9.31853056e-03,  1.80131018e-01,\n",
       "         5.27772121e-02, -1.06011704e-01, -9.23479348e-02,\n",
       "        -2.54552186e-01, -2.76538700e-01, -9.98182595e-02,\n",
       "        -1.47702947e-01, -3.28441322e-01,  1.25939548e-01,\n",
       "         7.30928630e-02,  3.42833363e-02, -3.50926876e-01,\n",
       "        -1.63926825e-01,  1.13887116e-02, -1.75869197e-01,\n",
       "         6.76653162e-02, -1.26846321e-02,  5.37508845e-01,\n",
       "         1.72573142e-02,  4.11000371e-01,  2.18482278e-02,\n",
       "        -8.83176550e-02,  3.00321162e-01, -7.29800314e-02,\n",
       "         5.31207919e-01, -2.27767050e-01,  1.64794877e-01,\n",
       "        -3.10520232e-01,  1.84387431e-01, -2.88293451e-01,\n",
       "        -3.30040693e-01, -2.30369374e-01, -3.46110672e-01,\n",
       "         2.99116969e-01,  1.80608913e-01, -1.05202362e-01,\n",
       "        -1.29004449e-01, -1.98854208e-01, -1.57072961e-01,\n",
       "         2.12674916e-01, -2.89036185e-01, -1.69108307e+00,\n",
       "         6.41600490e-01,  1.54779091e-01,  2.50405908e-01,\n",
       "        -1.39598578e-01, -9.41037834e-02,  3.22485305e-02,\n",
       "         4.62733954e-01,  2.07626507e-01,  1.58546790e-01,\n",
       "        -3.68520260e-01, -1.64966106e-01, -5.73239401e-02,\n",
       "         6.30119890e-02,  2.11461902e-01, -1.35530397e-01,\n",
       "        -1.15863293e-01, -1.13281794e-01, -9.45949256e-02,\n",
       "        -1.26294166e-01, -2.53288001e-01,  3.35531443e-01,\n",
       "         1.98025391e-01,  8.85341316e-03,  2.86453009e-01,\n",
       "        -1.82269320e-01,  5.88142611e-02,  4.20933008e-01,\n",
       "         8.04462582e-02,  5.80830932e-01, -1.63832232e-02,\n",
       "        -3.72927248e-01, -9.16775018e-02, -3.54519874e-01,\n",
       "         2.33174309e-01, -1.61196932e-01, -2.23777723e-02,\n",
       "        -1.03329822e-01,  3.07881236e-01,  2.85585940e-01,\n",
       "        -3.92440975e-01,  2.57726789e-01,  1.06674634e-01,\n",
       "         8.24644119e-02,  4.96255696e-01,  6.67017773e-02,\n",
       "        -5.12524992e-02,  1.00306161e-02, -1.10060945e-01,\n",
       "        -1.87841713e-01, -2.08728015e-04, -2.73076534e-01,\n",
       "        -1.09085798e-01, -3.18650723e-01, -1.18833609e-01,\n",
       "        -5.66026121e-02, -4.18341964e-01,  2.39480659e-01,\n",
       "        -3.07466269e-01, -9.95616615e-02,  1.28683299e-01,\n",
       "        -1.81389794e-01, -2.95796871e-01,  2.05032811e-01,\n",
       "        -9.40565765e-02, -6.33693159e-01, -2.84044519e-02,\n",
       "        -2.56793022e-01, -5.15416712e-02,  1.26287088e-01,\n",
       "         5.08261859e-01, -6.00182638e-03, -5.24103284e-01,\n",
       "         2.98925221e-01, -1.31859541e-01,  1.52070746e-01,\n",
       "         1.88292280e-01,  7.28498176e-02, -8.19140077e-02,\n",
       "        -1.39103681e-01, -8.65952894e-02, -6.62648737e-01,\n",
       "        -4.10586447e-01,  4.97705072e-01, -1.97434559e-01,\n",
       "         3.83696109e-01, -3.99890751e-01, -1.21460512e-01,\n",
       "         3.35064352e-01,  1.05290405e-01, -2.61185706e-01,\n",
       "        -2.50547439e-01, -1.13996863e-01,  1.55776739e-02,\n",
       "        -2.01231927e-01,  3.46779823e-04,  7.32920244e-02,\n",
       "        -9.67570916e-02, -6.08919337e-02, -9.71425027e-02,\n",
       "        -1.83021605e-01,  3.66620094e-01,  2.80040324e-01,\n",
       "         1.87838018e-01,  2.55652964e-02,  2.87159979e-02,\n",
       "         1.30316675e-01, -4.93010432e-02, -1.87979564e-01,\n",
       "         1.79201998e-02, -2.34443694e-03, -3.03666610e-02,\n",
       "        -1.53401978e-02,  8.55852589e-02,  3.85768339e-02,\n",
       "        -1.65195301e-01, -2.01810747e-01, -3.53873938e-01,\n",
       "         1.59596050e+00,  3.83640617e-01,  2.00509250e-01,\n",
       "        -1.07194319e-01,  2.84530669e-01, -2.61936672e-02,\n",
       "        -3.36764872e-01,  2.57682115e-01, -1.12204418e-01,\n",
       "         3.54397833e-01, -2.55355239e-01,  1.96272343e-01,\n",
       "        -3.19721192e-01,  1.29758716e-01,  5.60315669e-01,\n",
       "         2.64616430e-01,  2.88172513e-01, -4.09201652e-01,\n",
       "        -2.00372487e-01,  2.23080330e-02, -4.36751097e-01,\n",
       "         1.13050178e-01,  3.76291603e-01, -6.42804354e-02,\n",
       "        -2.57619806e-02,  1.91476136e-01,  2.45032921e-01,\n",
       "        -5.92863560e-02,  2.16083780e-01,  2.46107250e-01,\n",
       "        -1.16439149e-01,  4.56909500e-02,  4.43056643e-01,\n",
       "         6.48730695e-01, -3.20409685e-01, -6.11618012e-02,\n",
       "        -1.47372037e-02, -2.52252191e-01, -2.65759647e-01,\n",
       "         4.36729006e-03,  7.60980695e-02, -5.09144187e-01,\n",
       "         4.47885007e-01, -1.72135934e-01, -3.71363536e-02,\n",
       "         6.40417933e-01, -9.38703045e-02, -2.17067331e-01,\n",
       "         2.50166178e-01,  1.71362936e-01,  7.18962252e-02,\n",
       "         2.52627671e-01, -4.87082630e-01,  2.23444566e-01,\n",
       "         7.45161474e-02, -2.00240329e-01, -2.90190428e-01,\n",
       "        -1.49677530e-01, -1.17161646e-01,  2.21810997e-01,\n",
       "        -5.71349524e-02,  3.13152134e-01,  2.58159637e-01,\n",
       "         3.71132158e-02,  1.43715501e-01,  2.53905773e-01,\n",
       "        -2.12928265e-01,  2.57908612e-01,  2.26978466e-01,\n",
       "        -3.12696159e-01,  1.09419383e-01,  4.66882646e-01,\n",
       "         1.69717878e-01,  1.68373495e-01,  2.99273342e-01,\n",
       "         1.16669685e-01,  3.12677056e-01,  4.70739277e-03,\n",
       "        -2.27169767e-02, -1.44473290e+00,  3.88749093e-01,\n",
       "         1.18395992e-01,  1.11679971e-01, -2.81247310e-03,\n",
       "         2.64241993e-01,  2.86163688e-01, -1.31071121e-01,\n",
       "        -4.44730371e-03, -2.02509478e-01,  2.87409335e-01,\n",
       "         3.29347134e-01,  3.47259909e-01, -1.02385223e-01,\n",
       "         3.98033336e-02, -4.33033705e-01,  3.64148527e-01,\n",
       "        -3.44873786e-01, -4.98404920e-01, -1.25861794e-01,\n",
       "         1.04637161e-01,  3.70228529e-01,  1.75514936e-01,\n",
       "        -5.67877710e-01, -2.25328088e-01,  3.10995251e-01,\n",
       "        -5.82196340e-02, -4.09883112e-01,  1.20852172e-01,\n",
       "         1.11721143e-01, -7.70503134e-02,  2.92352736e-01,\n",
       "        -2.66647428e-01, -1.96076602e-01,  1.75403863e-01,\n",
       "        -1.48342118e-01, -2.14275360e-01, -1.25205249e-01,\n",
       "        -8.89889672e-02,  2.70950884e-01, -8.53752047e-02,\n",
       "         5.91911793e-01, -2.54815429e-01,  2.42254958e-01,\n",
       "        -6.81426302e-02,  1.96696624e-01,  3.81203085e-01,\n",
       "        -1.76095843e-01,  3.52412462e-01, -1.68743283e-01,\n",
       "        -3.26094776e-02,  1.01677574e-01,  5.44293523e-01,\n",
       "        -4.76242080e-02,  2.41561964e-01,  9.25374031e-02,\n",
       "         2.31816024e-01,  1.18713796e-01,  8.56270343e-02,\n",
       "        -3.36987823e-01, -7.98867345e-02, -8.76572728e-02,\n",
       "        -1.22862548e-01, -6.13774452e-03, -7.73486346e-02,\n",
       "        -6.17444664e-02, -2.60094672e-01, -1.04788065e-01,\n",
       "         3.29906493e-02, -4.93491083e-01, -2.22155675e-01,\n",
       "        -2.33779922e-01,  1.12137705e-01,  1.33985028e-01,\n",
       "         6.88631460e-02, -4.19367105e-01,  5.45900404e-01,\n",
       "         2.10366815e-01,  2.22935304e-01,  2.25555360e-01,\n",
       "         1.08169951e-01, -1.37498543e-01, -1.45699859e-01,\n",
       "        -7.01275319e-02,  2.40407363e-01, -4.61156750e+00,\n",
       "        -2.25908369e-01,  9.32200775e-02,  5.49106784e-02,\n",
       "        -3.79846722e-01, -4.78592604e-01, -1.24892779e-02,\n",
       "        -2.52394557e-01,  1.81402341e-02, -1.86301857e-01,\n",
       "         9.31275040e-02,  7.46774524e-02, -2.53475189e-01,\n",
       "        -2.40143076e-01,  4.75583762e-01,  2.26406902e-01]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers as ppb\n",
    "train_essays = data[:1]['essay']\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "train_features = get_features(tokenized_train)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2ypVnoH1m8D"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2Vfp2iOuOxGF",
    "outputId": "12837e9c-3d0b-4979-ddbd-2e3d941d6096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380,)\n",
      "(10380, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "trainDataVectors = np.reshape(train_features,(train_x,train_y,1))\n",
    "testDataVectors = np.reshape(test_features,(test_x,test_y,1))\n",
    "print(y_train.shape)\n",
    "print(trainDataVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "AqIKQXYaLS_r",
    "outputId": "3cddfc67-1121-41b0-aa2f-92582d2cd604"
   },
   "outputs": [],
   "source": [
    "# x = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "def plot_qwk_scores_all_sets():\n",
    "  fig = plt.figure()\n",
    "  ax = plt.subplot(111)\n",
    "  x = [1,2,3,4,5]\n",
    "  ax.plot(x, set1 , label='set1')\n",
    "  ax.plot(x, set2, label='set2')\n",
    "  ax.plot(x, set3, label='set3')\n",
    "  ax.plot(x, set4, label='set4')\n",
    "  ax.plot(x, set5, label='set5')\n",
    "  ax.plot(x, set6, label='set6')\n",
    "  ax.plot(x, set7, label='set7')\n",
    "  ax.plot(x, set8, label='set8')\n",
    "  plt.title('Set wise QWK using BERT for individual sets')\n",
    "  ax.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqFLucVADEps"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AES.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02f24c9a529f453ea521216a0009cca0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0634e813b3ce40d19138e79f02c073ba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0779784dd6bc4ac4b86bdb1a0681733c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09ccba4d9c3040d58b86daeeeef98202": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0ad3c14f5faa413fafc3cd5dba3c8082": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a5c8ebdcd884286bfe2bbdc739bab4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f100e48f3a1144c798acfb2ae4d546d2",
       "IPY_MODEL_8a6eb9f25c6b4fc38423b696bcf63862"
      ],
      "layout": "IPY_MODEL_237351b8013e465f8e8edfab0b40f81c"
     }
    },
    "237351b8013e465f8e8edfab0b40f81c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2654def4051f4fc58e7d2fb8c314365a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e690d7a41c743a09bbd7b9e3dcbff57",
       "IPY_MODEL_acb0089eba5a41d3a2fc5c3186bda706"
      ],
      "layout": "IPY_MODEL_02f24c9a529f453ea521216a0009cca0"
     }
    },
    "2ddbd58e2da34525847e40154241a378": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2e690d7a41c743a09bbd7b9e3dcbff57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c2db7a3d29748bb95a91095805a5390",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79a30aa163d943fa9c99be3ba26e4eed",
      "value": 231508
     }
    },
    "32f1b625e39349cbbcb9ed38ea3fb2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_926f21ae08c544629beb194d9a278fc3",
      "placeholder": "​",
      "style": "IPY_MODEL_b376e92a82ff4658b1bcf53a712ceaef",
      "value": " 546/546 [00:09&lt;00:00, 60.4B/s]"
     }
    },
    "363064ed63c44d7bbffc4a2c894c0090": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c2db7a3d29748bb95a91095805a5390": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e6f729e131b468585957f5b8a9f3d69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5656069392ca4571a684592ac8249b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e32a632b2eb4add97fb516443563359": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b60be783b9bf4a418a08416b556a53b4",
      "max": 546,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9901c7aa3a34059b5e2c89deeac139f",
      "value": 546
     }
    },
    "7393714715c742f7b3958730e9358cb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db76e192c7494af08e81baeab080f025",
       "IPY_MODEL_aa901d9e3c2349f787ac4f86216b8c18"
      ],
      "layout": "IPY_MODEL_91ddadae764941ceb891f87234a5263e"
     }
    },
    "76593de64e9c4d87bfa2946e82fe5193": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79a30aa163d943fa9c99be3ba26e4eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7a5ba63f75a544518fbb1f3afffec561": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f6636125e0749e0943e49e8b63e8852": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82c96ecfea1d41769acbdf18468cb08e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a6eb9f25c6b4fc38423b696bcf63862": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a20a8ee8e1ba4eee850775943c1d8d15",
      "placeholder": "​",
      "style": "IPY_MODEL_7f6636125e0749e0943e49e8b63e8852",
      "value": " 268M/268M [00:08&lt;00:00, 32.1MB/s]"
     }
    },
    "8adca1280df0497fa40c4b90093982f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8ed4c00e67fd4dbfa46731f82a2b943b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_363064ed63c44d7bbffc4a2c894c0090",
      "placeholder": "​",
      "style": "IPY_MODEL_5656069392ca4571a684592ac8249b27",
      "value": " 546/546 [00:10&lt;00:00, 54.6B/s]"
     }
    },
    "91ddadae764941ceb891f87234a5263e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "926f21ae08c544629beb194d9a278fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20a8ee8e1ba4eee850775943c1d8d15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4581768d8e0411ba420f4cf24cb42b5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4c766d6c47347c6a8a9c28f54f1e76b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5b70a5b43ae4e7092e5449279fe6680": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aa901d9e3c2349f787ac4f86216b8c18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4581768d8e0411ba420f4cf24cb42b5",
      "placeholder": "​",
      "style": "IPY_MODEL_c1d9c56e0ac44db58f5d7831135680df",
      "value": " 268M/268M [00:09&lt;00:00, 29.1MB/s]"
     }
    },
    "acb0089eba5a41d3a2fc5c3186bda706": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76593de64e9c4d87bfa2946e82fe5193",
      "placeholder": "​",
      "style": "IPY_MODEL_0779784dd6bc4ac4b86bdb1a0681733c",
      "value": " 232k/232k [00:00&lt;00:00, 797kB/s]"
     }
    },
    "b376e92a82ff4658b1bcf53a712ceaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b60be783b9bf4a418a08416b556a53b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bddbbc95b581427db7095e07781ed076": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ad3c14f5faa413fafc3cd5dba3c8082",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09ccba4d9c3040d58b86daeeeef98202",
      "value": 231508
     }
    },
    "c1d9c56e0ac44db58f5d7831135680df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c367d14c83cc443dbcfd794cf180c998": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e32a632b2eb4add97fb516443563359",
       "IPY_MODEL_8ed4c00e67fd4dbfa46731f82a2b943b"
      ],
      "layout": "IPY_MODEL_7a5ba63f75a544518fbb1f3afffec561"
     }
    },
    "d6a67032d4cb46c88d0058bb7d3fe398": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e695a2f265bd496b9ad8e3afcecd1ea8",
       "IPY_MODEL_32f1b625e39349cbbcb9ed38ea3fb2ae"
      ],
      "layout": "IPY_MODEL_f2fbe75c769f43c28265687fdf2fe9ad"
     }
    },
    "d9901c7aa3a34059b5e2c89deeac139f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "db76e192c7494af08e81baeab080f025": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4c766d6c47347c6a8a9c28f54f1e76b",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ddbd58e2da34525847e40154241a378",
      "value": 267967963
     }
    },
    "e695a2f265bd496b9ad8e3afcecd1ea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6a72387356b4133ba9c1bf29f6bf368",
      "max": 546,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8adca1280df0497fa40c4b90093982f9",
      "value": 546
     }
    },
    "e6a72387356b4133ba9c1bf29f6bf368": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7698d97b5f84c429344ec111f418fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e6f729e131b468585957f5b8a9f3d69",
      "placeholder": "​",
      "style": "IPY_MODEL_fd9f1b4bcae24ff98cab4af7ce4811c0",
      "value": " 232k/232k [00:00&lt;00:00, 637kB/s]"
     }
    },
    "f100e48f3a1144c798acfb2ae4d546d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82c96ecfea1d41769acbdf18468cb08e",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5b70a5b43ae4e7092e5449279fe6680",
      "value": 267967963
     }
    },
    "f2fbe75c769f43c28265687fdf2fe9ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9a8c9990ce64e25bfa344145d438bb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bddbbc95b581427db7095e07781ed076",
       "IPY_MODEL_e7698d97b5f84c429344ec111f418fc6"
      ],
      "layout": "IPY_MODEL_0634e813b3ce40d19138e79f02c073ba"
     }
    },
    "fd9f1b4bcae24ff98cab4af7ce4811c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}